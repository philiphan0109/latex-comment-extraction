\section{Related work}
\label{related_work}

While RL as a field has seen many innovations in the last
years, small changes to the algorithm or its implementation can have a big impact on its results \citep{engstrom2020implementation, joajo2021lifting}.  Deep reinforcement learning approaches are often notoriously sensitive to their hyperparamaters and demonstrate brittle convergence properties \citep{haarnoja2018soft}. This is particularly true for off-policy approaches that use a replay buffer to leverage past experiences \citep{duan2016benchmarking}.


\cite{Henderson2017DeepRL} investigate the effects of existing degrees of variability between various RL setups and their effects on algorithm performance. Although restricted to the domain of existing environments, \cite{Henderson2017DeepRL} propose more robust performance estimators for RL learning algorithms. \cite{islam2017reproducibility} and \cite{shengyi2022the37implementation} have shown the difficulty in reproducing policy gradient algorithms due to the variance.
\cite{andrychowicz2020matters} did a deep dive in algorithmic choices on policy-based algorithms. Their analyses covered differences in hyper-parameters, algorithms, and implementation details.


In an effort to consolidate innovations in deep RL, several papers have examined the effect of smaller design decisions like the loss function or policy regularization for on-policy algorithms \cite{andrychowicz2020matters}, DQN agents \citep{obando2020revisiting}, imitation learning \citep{hussenot2021hyperparameter} and offline RL \citep{paine2020hyperparameter, lu2021revisiting}. AutoRL methods, on the other hand, have focused on automating and abstracting some of these decisions \citep{parker2022automated, eimer2023hyperparameters} by using data-driven approaches to learn various algorithmic components or even entire RL algorithms \citep{co2021evolving,lu2022discovered}. All these works have demonstrated that hyperparameters in deep reinforcement learning warrant more attention from the research community than they currently receive. Underreported tuning practices can distort algorithm evaluations, and overlooked hyperparameters may lead to suboptimal performance.


% Despite all the efforts, there are still some mysteries that have been not understood yet. Many of these new and unexpected discoveries have remained concealed due to limited parameter exploration caused by the substantial computational resources they demand. Investigate the impact of design choices and hyper-parameter in deep RL algorithms with large state environments is very challenging and almost impossible to explore on academic labs. Therefore, we decide to focus on exploring the relationship between hyparparemeters and  value-based methods, as opposed to the greater focus in Actor-Critic and Policy-based of the previous works \citep{andrychowicz2020matters}.
