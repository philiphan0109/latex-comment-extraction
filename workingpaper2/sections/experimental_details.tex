\section{THC Score}
\label{sec:thc_metric}

Statistical metrics play a crucial role in assessing and evaluating the performance of DRL algorithms. 
They provide valuable insights into the strengths and weaknesses of different approaches, guiding researchers and practitioners in the development of more effective reinforcement learning systems.
For example, some the metrics focus on the mean reward obtained by an agent per time step (Average Reward), the percentage of episodes in which the agent achieves a predefined goal or task (success rate) among others \citep{agarwal2021deep, chan2020measuring, Henderson2017DeepRL}. 

Measuring the transferability/consistency of hyper-parameters in DRL is challenging, as existing metrics fall short in capturing the nuanced aspects of how well hyper-parameter settings generalize across different environments or agents. Developing such a metric would enhance the ability to systematically compare and select hyper-parameter configurations that exhibit robust performance across a range of application domains.

To understand the consistency of hyper-parameters we focus on their ranking consistency across experimental settings. Put another way: if a given hyper-parameter value is optimal/pessimal in a setting, is it still optimal/pessimal in another? And so we analyse, for each hyper-parameter, whether their values lead to the same ranking order for different experimental settings, where the ranking is on final performance. 

We compute ranking agreement for three setups: 
{\bf $1$) Varying algorithms} while keeping the environment and data regime fixed (e.g. when proposing a new value-based algorithm but not having enough compute to run a comprehensive hyper-parameter search). {\bf $2$) Varying environments} while keeping the algorithm and data regime fixed (e.g. when using a state of the art algorithm in a new domain).
{\bf $3$) Varying data regimes} while keeping the environment and algorithm fixed (e.g. when adapting a new algorithm to a new data regime \citep{hasselt19when}).
Concretely, our desire is to have a metric that yields a high value score would indicate that the hyper-parameter in question is {\em important}, in the sense that it will likely require retuning; conversely, a low score suggests the hyper-parameter value can likely be kept as is.

Kendall's Tau \citep{kendall38measure} and Kendall's W \citep{10.1214/aoms/1177732186} are natural choices, but these metrics were developed for situations where the rankings were based on a single score, instead of a range of possible scores, and they can result in degenerate values when two settings have similar performance or when two settings alternate between optimal and pessimal rankings. For these reasons, we introduce the \textbf{T}uning \textbf{H}yperparameter \textbf{C}onsistency ({\bf THC}) score. Consider a set of $n$ hyper-parameters $\lbrace H_1,\ldots,H_n\rbrace$, each with its set of values $\lbrace\lbrace h_{11},h_{12},\ldots,h_{1m_1}\rbrace, \ldots,\lbrace h_{n1},h_{n2},\ldots,h_{nm_n}\rbrace\rbrace$ (e.g. hyper-parameter $H_i$ has $m_i$ values). The THC score involves three computations: (i) rankings for each hyper-parameter setting (\autoref{alg:computeRankings}); (ii) normalized peak-to-peak value for each hyper-parameter setting (Eqn.~\ref{eqn:ptp} below); and (iii) overall THC score for the hyper-parameter (see Eqn.~\ref{eqn:thc} below).

If we run multiple independent runs for each hyper-parameter setting $h_{ij}$, we can compute the mean $\mu_{ij}$ and standard deviation $\sigma_{ij}$ for these runs\footnote{One may also use confidence intervals instead of standard deviations.}. For each hyper-parameter setting $h_{ij}$ we then compute an initial ranking $r'_{ij}$ based on the upper bound ($\mu_{ij}+\sigma_{ij}$), with the lower bound ($\mu_{ij}-\sigma_{ij}$) used to break ties. We then define a set containing hyper-parameter settings with overlapping values:
\begin{align*}
    I_{ij} := \{k \vert (\mu_{ij} - \sigma_{ij} < \mu_{ik} + \sigma_{ik} &\text{ and } \mu_{ij} - \sigma_{ij} > \mu_{ik} - \sigma_{ik}) \\ &\text{ or } \\ \break (\mu_{ij} + \sigma_{ij} > \mu_{ik} - \sigma_{ik} &\text{ and } \mu_{ij} + \sigma_{ij} < \mu_{ik} + \sigma_{ik}) \}
\end{align*}



\begin{algorithm}[!t]
\caption{Compute rankings}\label{alg:computeRankings}
\begin{algorithmic}[1]
\Require Multiple runs for various settings of hyper-parameter $H_i$: $\lbrace h_{i1},h_{i2},\ldots,h_{im_i}\rbrace$, aggregate metrics $\mu_i$: $\lbrace \mu_{i1},\mu_{i2},\ldots,\mu_{im_i}\rbrace$ and measure of spread $\sigma_i$: $\lbrace \sigma_{i1},\sigma_{i2},\ldots,\sigma_{im_i}\rbrace$
\For{$i$ in $1 \ldots n$}
    \State $r'_{i} = \textrm{argsort}(\mu_i + \sigma_i)$ \Comment{Gets the index of each value as if the array was sorted}
    \State $\mu'_i, \sigma'_i = \mu_i[r'_{i}], \sigma_i[r'_{i}]$ \Comment{Sorted versions of aggregate and spread metrics}
    \For{$j$ in $1 \ldots m_{i}$} 
        \State $u_{j} = \textrm{binary\_search}(\mu'_i - \sigma'_i, \mu_{ij} + \sigma_{ij})$ \Comment{highest rank whose lower bound overlaps with j}
        \State $l_{j} = \textrm{binary\_search}(\mu'_i + \sigma'_i, \mu_{ij} - \sigma_{ij})$ \Comment{lowest rank whose upper bound overlaps with j}
    \EndFor
    \State $\bf{r_{i}} = \frac{u + l}{2}$ \Comment{The average rank in $l_j,l_j+1, \ldots, u_j$ is the average of $l_j$ and $u_j$}
\EndFor

\end{algorithmic}
\end{algorithm}

The final ranking of each hyper-parameter is $r_{ij} = \frac{\sum_{k \in I_{ij}} r
'_{ik}}{\vert I_{ij} \vert}$, 
as \autoref{alg:computeRankings} details. These rankings are for {\em one} training regime; however, as mentioned in the introduction, we are interested in quantifying the {\em consistency} of a hyper-parameter $H$ across varying training regimes. Consider four training regimes $A, B, C, D$, and let $\lbrace \mathfrak{R}^A,\ldots,\mathfrak{R}^D\rbrace$ denote their respective rankings. For each hyper-parameter value $h_x\in H$ we compute its normalized ``peak-to-peak''\footnote{Inspired by numpy's peak-to-peak function numpy.ptp \citep{harris2020array}.} value $\overline{\textrm{ptp}}$, which quantifies its variance in ranking, as follows: First compute the $\textrm{ptp}$ value $\textrm{ptp}(h_x) = \max\left(\lbrace \mathfrak{R}^A(h_x),\ldots,\mathfrak{R}^D(h_x)\rbrace\right) - \min\left(\lbrace \mathfrak{R}^A(h_x),\ldots,\mathfrak{R}^D(h_x)\rbrace\right)$, then normalize:
\begin{align}
    \overline{\textrm{ptp}}(h_x) = \frac{\textrm{ptp}(h_x)}{\sum_{h_y\in H}\textrm{ptp}(h_y)}
    \label{eqn:ptp}
\end{align}

Notably, hyper-parameter settings that have consistent rankings across training regimes will have a normalized $\textrm{ptp}$ value of zero. Finally, the $\textrm{THC}$ score for hyper-parameter $H$ is defined as:
\begin{align}
    \textrm{THC}(H) = \frac{\sum_{h_x\in H}\overline{\textrm{ptp}}(h_x)}{|H|}.
    \label{eqn:thc}
\end{align}

This score will result in low values for hyper-parameters whose varying settings have consistent ranking across various training regimes, and high values when these rankings vary. Intuitively, {\em hyper-parameters with high values will most likely require re-tuning when switching training regimes}. See \autoref{sec:appendixTHC} for more examples of computing the score, as well as the source code provided with this submission.


