\section{Background}
\label{sec:brackground}

% We cover the necessary background in a little more detail than is traditionally done, so as to be able to draw a direct connection to the hyper-parameters considered. 

The field of reinforcement learning studies algorithms for sequential decision-making problems. In these settings, an algorithm (or agent) interacts with an {\em environment} by transitioning between {\em states} and making action choices at discrete timesteps; the environment responds to each action by (possibly) changing the agent's state and yielding a numerical reward or cost. The goal of the agent is to maximize the cumulative rewards (or minimize the cost) throughout its lifetime.
This is typically formalized as a Markov decision process (MDP) \citep{puterman2014markov} $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\mathcal{X}$ is the set of states, $\mathcal{A}$ is the set of available actions, $\mathcal{P}:\mathcal{X}\times\mathcal{A}\rightarrow \Delta(\mathcal{X})$\footnote{$\Delta(X)$ denotes a distribution over the set $X$.} is the transition function, $\mathcal{R}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}$ is the reward function, and $\gamma\in [0, 1)$ is a discount factor. An agent's behaviour is formalized by a policy $\pi:\mathcal{X}\rightarrow\Delta(\mathcal{A})$, whose {\em value} from any state $x\in\mathcal{X}$ is given by the Bellman recurrence 
$V^{\pi}(x) := \mathbb{E}_{a\sim\pi(x)}\left[\mathcal{R}(x, a) + \gamma \mathbb{E}_{x'\sim\mathcal{P}(x, a)}V^{\pi}(x')\right]$. $Q$-functions allow us to measure the value of taking any action $a\in\mathcal{A}$ from a state $x\in\mathcal{X}$ and following $\pi$ afterwards: $Q^{\pi}(x, a) := \mathcal{R}(x, a) + \gamma \mathbb{E}_{x'\sim\mathcal{P}(x, a)} V^{\pi}(x')$. %This admits a simple algorithm for defining a new policy $\pi^\prime$ that is at least as good as $\pi$: $\pi^\prime(x) = \arg\max_{a\in\mathcal{A}}Q^{\pi}(x, a)$. 
A policy $\pi^*$ is considered optimal if for any policy $\pi$, $V^* := V^{\pi^*} \geq V^{\pi}$.

Solving for the equations discussed above would require access to both $\mathcal{R}$ and $\mathcal{P}$, which are usually unknown. Instead, RL typically assumes the agent has access to transitions $\tau := (x, a, r, x')\in\mathcal{X}\times\mathcal{A}\times\mathbb{R}\times\mathcal{X}$, arising from interactions with the environment. Given such a transition,  $Q$-learning \citep{Watkins1992qlearning} updates its estimate of $Q$ via: $Q_{t+1}(x, a) \leftarrow  Q_t(x, a) + {\alpha} TD(Q, \tau)$, where {$\alpha$} is a learning rate and $TD$ is the {\em temporal-difference error}, given by $TD(Q_t, \tau) := r + \gamma \max_{a' \in\mathcal{A}} Q_t(x', a') - Q_t(x, a)$. If the state and action spaces are small, one can store all the $Q$-values in a table of size $|\mathcal{X}|\times |\mathcal{A}|$. For most problems of interest, however, state spaces are very large (and possibly infinite). In these cases, one can use a function approximator, such as a neural network, parameterized by $\theta$: $Q_\theta\approx Q$. Indeed, in order to achieve super-human performance on the Arcade Learning Environment (ALE) \citep{bellemare2012ale}, \citet{mnih2015humanlevel} used a neural network consisting of three convolutional layers (Conv layers), followed by two multi-layer perceptrons (Dense layers) with $|\mathcal{A}|$ outputs in the final layer (representing the $Q$-value estimates for each action). With the exception of the final layer, a ReLU non-linearity follows each layer.

Updating $Q_{\theta}$ thus corresponds to updating the parameters $\theta$, which may be done by using optimization algorithms such as Adam \citep{kingma15adam} to minimize the temporal-difference error. At a high-level, this yields an update of the form: $\theta_{t+1} \leftarrow \theta_t + \alpha\nabla_{\theta_t} \mathbb{E}_{\tau\sim\mathcal{D}} TD(Q_{\theta_t}, \tau)$.
The expectation can be approximated using a batch of $m$ transitions drawn from a distribution $\mathcal{D}$, which can be computed efficiently on specialized hardware such as GPUs and TPUs. Additionally, \citet{mnih2015humanlevel} argued that using $\bar{\theta}$, a less-frequently updated copy of the parameters, when computing TD helps with training stability. A common approach introduced by \citet{mnih2015humanlevel} is to clip the rewards at $(-1, 1)$. The TD term thus becomes:
  $TD(Q_{\theta}, \tau) := clip(r, (-1, 1)) + \gamma\max_{a'\in\mathcal{A}}Q_{\bar{\theta}}(x', a') - Q_{\theta}(x, a)$.

Although DQN benchmarked on the 57 ALE games with the same set of hyper-parameters, \citet{anschel2017averageddqn} 
%and \citet{cini2020deep} 
demonstrated that in some environments it can result in degraded performance.
A number of papers have proposed improvements to increase stability and performance, which \citet{Hessel2018RainbowCI} combined into a single agent they called \emph{Rainbow}. Specifically, they combined DQN with double Q-learning \citep{hasselt2015doubledqn}, prioritized experience replay \citep{Schaul2016PrioritizedER}, dueling networks \citep{wang16dueling}, multi-step learning \citep{sutton88learning}, noisy nets \citep{fortunato18noisy}, and distributional reinforcement learning \citep{Bellemare2017ADP}.

