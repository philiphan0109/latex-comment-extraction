\section{Introduction}
Mental health pertains to emotional, psychological, and social well-being, influencing daily thoughts, feelings, and actions. Mental illness is a leading cause of disability, with an estimated 450 million people affected worldwide \cite{world2003investing}. It is also a significant predictor of suicide \cite{nock2010mental}. Mental disorders usually emerge in an individual's early 20s \cite{kessler2005lifetime}, and their untreated presence can negatively impact academic success, productivity, and social relationships \cite{kessler1995social, wang2007telephone}. In the context of COVID-19, the need for social distancing has led to the widespread adoption of telehealth services such as telepsychiatry \cite{telehealthscoping, oâ€™brien_mcnicholas_telepsych}. Unfortunately, many individuals are experiencing mental health issues during the pandemic, with the highest levels of pandemic-era anxiety and depression observed in 2020 across all age groups, which began to decline in early 2021 \cite{villaume2023age}. While many COVID-19 restrictions have been lifted, approximately 5\% of the U.S. adult population, or about 12 million Americans, are living with co-occurring chronic pain and clinically significant symptoms of anxiety and depression \cite{jennifer2024co}. This ongoing situation underscores the importance of reconsidering how to deliver mental health care effectively at the right time. Personalized psychiatric care, which promotes preventive measures and offers tailored interventions, could help meet these needs, though its availability remains limited \cite{abdullah2018sensing}.

A growing body of psychological studies \cite{ellgring2007non, girard2013social, girard2014nonverbal, stuhrmann2011facial} have suggested that depression is characterized by nonverbal signals such as facial muscle movement, and head gesture, which can be detected automatically without the need for clinical intervention. We refer to these as \textit{"facial behavior primitives"}. Research has shown that mental illness, such as depression, leaves recognizable markers in the facial patterns of an individual \cite{samareh2018detect}. Often, these changes manifest in a person's face involuntarily. Creating an automatic system \cite{cohn2009detecting, song_spectral_2020, valstar2014avec} based on these cues can provide an objective and repeatable evaluation and address problems related to cost and time requirements. Despite the valuable insights gained from these studies, it should be noted that they were conducted in controlled lab environments and recorded videos of individuals' faces. Currently, the real-life implementation of such systems is limited due to privacy concerns \cite{cohn2009detecting, valstar2013avec, song2020spectral, kong2022automatic, casado2023depression}, unrealistic costs \cite{pedrelli2020monitoring}, and required computational power \cite{chikersal2021detecting, ferreira2015aware, opoku2022mood}. 


While facial actions have shown promise in lab settings for understanding depression, their application in real-world scenarios remains largely unexplored due to challenges in designing efficient, deployable mobile systems. To bridge this gap, we introduce FacePsy, an open-source mobile sensing system capturing facial features, generating real-time data on facial behavior landmarks, eye open, smile, and head gestures, all through smartphones in natural settings, while preserving user privacy. We hypothesize that digital biomarkers extracted from facial cues offer valuable insights into an individual's internal emotional and affective state, thereby enabling algorithms to infer depression. In this field study, we gathered data in real-world contexts to assess how and whether the data collected through our framework could demonstrate the potential for predicting depressive episodes in naturalistic environments.


While there are studies that use mobile sensing to track patterns in social and behavioral data by tracking communications, app usage, and GPS data \cite{wang2014studentlife, chikersal2021detecting, opoku2022mood}, these mobile sensing-based solutions primarily focus on capturing social and behavioral data but disregarding affective signals, which have been shown to be important indicators of depression. These studies have particular challenges and barriers: (1) Mobile sensing limits modeling usage because it requires extensive computation and post-processing. The burden of collecting sensors 24/7 and battery consumption may lead to low compliance. It might not be usable for stakeholders. (2) Wang et al. \cite{wang2015using} tried to capture entire face images in the real-world settings to understand depression but reported that they failed to validate the effectiveness of data due to insufficient frames to build a model (one frame when unlocking the smartphone ); (3) Tseng et al. collected and analyzed eye patches in detecting alertness \cite{tseng_alertnessscanner_2018} not depression, but partially captured a part of the face only (eye). Most recently, MoodCapture \cite{nepal2024moodcapture} was introduced that captures facial images in natural environments for depression detection. This involves analyzing image attributes such as angle, dominant colors, location, objects, and lighting. The utility of MoodCapture for developers seeking to implement similar studies in different settings may be somewhat limited, as the authors have not made their mobile system, dataset, or machine learning pipeline publicly available. Our study complements MoodCapture's work by exploring the incremental utility of mobile sensing for depression detection and advocating for new avenues to develop mental health assessment tools based on in-the-wild images. Our research advances from MoodCapture in terms of data collection mechanisms, on-device processing, privacy awareness, and the facial attributes collected. While MoodCapture collects facial data when a user responds to survey questions, our study implements a trigger-based data collection motivated by prior literature \cite{tseng2018alertnessscanner}. This mechanism activates based on user actions, such as turning the screen on/off, opening/closing apps, etc., to start or stop data collection. For on-device processing, we only send the final detected facial behavior primitives (Action Units (AU), smile, eye open state, head Euler angles, and landmarks) to the research server for further analysis. This helps us ensure user privacy and prevent the leakage of facial images by discarding images from user device after processing. Our work also advocates for privacy-aware data collection. This approach is informed by prior literature on nudging \cite{balebako2014improving}, informing users about active data collection \cite{felt2012ve, denning2014situ}, and notifying users when the app restarts itself after a reboot or crash \cite{denning2014situ}. While MoodCapture has first introduced the use of facial images for depression detection in natural environments, our approach proposes a novel facet to this field in several key aspects. We are among the first to develop an open-sourced, privacy-aware, trigger-based affective mobile system that captures facial data from users' smartphones and immediately discards the raw images after extracting essential features in near real-time (within 10 seconds). This method not only builds a predictive model of depressive episodes but also ensures that sensitive facial data is not permanently stored on devices, addressing significant privacy concerns and awareness.


To advance affective computing systems, making it applicable in real-world settings, we synthesize a set of affective signals from face which have been well-validated such as facial muscular activities (AUs) as well as proposed new features beyond simple facial expression algorithms that have been unexplored and invalidated in a user's everyday settings. Research questions as follows: (RQ1) What are the important signals of affective biomarkers on depressive episode detection by differentiating depressive and non-depressive episodes, and how can those key features contribute to the model's performance? and (RQ2) Whether and how can an affective mobile system be efficiently designed, tested, and developed to understand a user's mental health, specifically in predicting depressive episodes, in real-world settings? As a result, we identified specific affective indicators as crucial factors for distinguishing between individuals experiencing depressive and non-depressive episodes. These indicators encompass the eye-open state, head pose, smile expression, and specific Action Units (2, 6, 7, 12, 15, and 17). When these features are combined, they exhibit predictive potential for detecting depression episodes, achieving an AUROC of 67\% for universal model, while the hybrid model has an AUROC of 81\%. It is worth noting that further enhancements in predictive accuracy can be attained through the accumulation of additional data spanning subsequent weeks. These findings represent a significant stride in bridging the existing disparity between controlled laboratory studies and the practical implementation of depression detection through affective mobile sensing systems in real-world scenarios.

As such, we have developed a deployable and usable open-sourced, lightweight, affective mobile system for the HCI community \footnote{{Our system source code is available at: https://github.com/stevenshci/FacePsy}}. Our system integrates state-of-the-art facial biomarkers, which have been validated to understand complex mental states and workloads in controlled lab settings. We have further expanded these features for the context of depression detection. The system automatically extracts these features from a user's smartphone in natural environments. This system has the potential to create new avenues for developing mental health assessment tools and behavior modeling based on in-the-wild images. Moreover, our FacePsy system can be deployed in everyday settings, and it is optimized with a sampling rate of 2.5 FPS without any delays on the user's own phones. Further, we provide insights with the experiments with different subset of facial behavior primitives features in detecting depression in naturalistic environments. As we highlight the impact of each subset of features validated in naturalistic environments, researchers and developers can utilize our mobile system to conduct their studies, configure apps for triggering time and frequency, and build their own computational models.

