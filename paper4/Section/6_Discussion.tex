\section{Lessons Learned: Discussing the Detection of Depressive Episodes on a Scale}
In this section, we highlight our work that contributes to new insights into the detection of depressive episodes in everyday settings by designing and developing an open-source affective mobile system. We found eye open state, head pose, smile, and action units (2, 6, 7, 12, 15, and 17) as key affective indicators in differentiating depressive and non-depressive episodes that have been validated in our field study. The combined features can be used to predict depression episodes. The universal model has an AUROC of 67\%, while the hybrid model has an AUROC of 81\%. Further improvement can achieved by collecting more data for subsequent weeks. These results serve as a bridge between controlled laboratory studies and real-world applications, demonstrating the feasibility of depression detection using an affective mobile sensing system.
% Given these results, we bridge the gap between controlled lab studies and real-world applications in depression detection feasibility with affective mobile sensing system.

\subsection{Comparison to Prior Work}
In the landscape of depression detection using mobile sensing, our approach significantly advances by leveraging in-the-wild data collection through everyday smartphone interactions. This method contrasts with many previous studies that primarily utilize lab-controlled \cite{cohn2009detecting, valstar2013avec, song2020spectral, kong2022automatic, casado2023depression} environments for data collection using camera modality, thus limiting the generalizability of their findings. For instance, our work complements the findings of Nepal et al. \cite{nepal2024moodcapture}, which also employs an in-the-wild approach but focuses on different feature sets, such as gaze and head pose, along with 2D and 3D facial landmarks. While their study achieves a balanced accuracy of 61\%, our method enhances the model's sensitivity to subtler indicators of depression through a detailed analysis of facial action units (AUs), eye-open states, and smile expressions. Our method achieves an AUROC of 81\% and Accuracy of 69\%, and maintains a consistent MAE across different validation strategies, highlighting the robustness of our findings and their potential for real-world application. This demonstrates the incremental utility of our approach, particularly in the seamless integration of mental health monitoring into daily technology use, thus contributing valuable insights into mobile health (mHealth) technologies for depression detection.

In comparison to mobile sensing, studies focused on depression detection, our approach capitalizes on direct behavioral markers accessible via a smartphone camera, distinguishing it from studies that rely on peripheral sensor data such as GPS or physical activity metrics. For example, Chikersal et al. \cite{chikersal2021detecting} and Opoku et al. \cite{opoku2022mood} employ a variety of sensors to infer depressive states indirectly through changes in mobility patterns and phone usage. While these studies achieve high accuracy and AUC scores, they may not capture the nuanced emotional states that facial behavior can indicate. Our method's utilization of detailed facial action units, eye-open states, and head gestures offers a more direct and potentially insightful measure of depressive episodes, evidenced by our comparable AUC of 81\%. Our model outperformed compared to a model that solely utilized sensor data from the AWARE platform \cite{ferreira2015aware}, which included Bluetooth (Accuracy=69.3, F1=0.64), Calls (Accuracy=68.5, F1=0.59), GPS (Accuracy=69.5, F1=0.62), and Steps Counter (Accuracy=63.6, F1=0.53), as detailed in research by Chikersal et al. \cite{chikersal2021detecting}. Our approach achieved an accuracy of 69\% and an F1 score of 0.67, surpassing individual sensor results. However, Chikersal et al. reported a higher F1 score of 0.78 when combining all sensors, indicating superior performance compared to our model in that specific setup. This specificity in detecting emotional expressions offers a critical enhancement over traditional mobile sensing methods, making our approach a valuable addition to the spectrum of technologies for monitoring mental health in everyday settings.

\subsection{Insights, Challenges and Opportunities in Predicting Depressive Episodes in the Naturalistic Environments}
Previous lab-controlled studies have delved into the extraction of facial behavior primitives from face images using affect sensing systems like OpenFace \cite{baltrusaitis2018openface}. These studies \cite{cohn2009detecting, valstar2013avec, song2020spectral, kong2022automatic, casado2023depression} have achieved impressive performance in detecting depression, due to their high data collection rate (e.g., processing video) and the controlled environment in which the data was collected. In contrast, deploying these systems in real-world scenarios using smartphones results in several challenges. On-device resource limitations often lead to a reduced frame rate (2.5Hz), and the unconstrained data collection settings -- affected by factors such as varied lighting conditions, phone orientation, ongoing activities, and whether the environment is indoors or outdoors -- can lead to poor quality of data collection. This, in turn, can impact the lack of samples that limits the development of predictive machine learning. Most recently, MoodCapture \cite{nepal2024moodcapture} was introduced that captures facial images in natural environments for depression detection. Their research demonstrated that using a random forest algorithm trained on facial landmarks, it's possible to identify depression and predict PHQ-8 scores among individuals. However, the utility of MoodCapture for developers intending to replicate such studies in different settings may be limited due to the lack of access to the mobile system, dataset, or machine learning framework used by the authors. Our research builds upon MoodCapture's work; we advance data collection by using a literature-based trigger mechanism that responds to user interactions like screen activity and app usage, enhancing user privacy by processing data on-device and discarding raw images post-analysis. Our study is novel in developing an open-source, privacy-aware mobile system that captures and processes facial data in near real-time, introducing significant improvements in privacy-awareness, data collection, and on-device processing.

The similarities and differences observed between lab-controlled and real-world predictive models can be attributed to the inherent nature of the environments in which they operate. Grounded in HCI/affective computing theories \cite{picard1999affective}, controlled environments allow for minimizing external variables \cite{campbell2015experimental}, ensuring that the subject's emotional state primarily influences the data collected. In such settings, the system can focus solely on the facial behavior primitives without interfering with external factors. However, in real-world scenarios, the myriad of uncontrollable variables, from lighting to personal activities, introduces noise into the data, which can mask or distort the true emotional indicators. From an affective computing perspective, human-computer interaction is dynamic and multifaceted in real-world settings. The system has to interpret the emotional state and account for the context in which the interaction is taking place. This context can significantly influence the emotional indicators, making them more complex to decipher. These insights are crucial for the design of an affective mobile framework for mental health. Recognizing the challenges of real-world data collection, future frameworks should incorporate adaptive algorithms that can adjust to varying conditions, ensuring consistent and accurate predictions. Additionally, leveraging context-aware computing can help the system contextualize the data, distinguishing between genuine emotional indicators and those influenced by external factors. This approach would lead to a more robust and reliable affective mobile sensing system, enhancing its potential in mental health applications.

Our method demonstrates good performance in depression detection compared to previous work by Opoku et al. \cite{opoku2022mood}, which used a similar learning scheme for a hybrid model. However, it's important to note that this comparison may not be entirely direct. Additionally, our universal model shows fair performance compared to prior work by Chikersal et al. \cite{chikersal2021detecting} in passive sensing using a similar learning scheme. The referenced study incorporates a comprehensive set of features, including sleep patterns, physical activity, phone usage, GPS location, and daily mood ratings, to model behavioral and social signals. However, as extensive research suggests, depression is a multifaceted disorder that affects various aspects of an individual's behavior, social interactions, and physiology. The existing mobile sensing-based approaches \cite{opoku2022mood, chikersal2021detecting, farhan2016behavior} have limitations in capturing affective signals \cite{abdullah2018sensing} manifested through involuntary facial muscle and head gestures, which have been established as crucial indicators of depression. Although wearable sensing-based physiological solutions have been proposed, their high deployment costs including extra cost for purchasing and wearing devices present a significant challenge and lack of affect/emotional signals. Previous research in affective computing conducted within controlled laboratory settings \cite{cohn2009detecting, valstar2013avec, song2020spectral, kong2022automatic, casado2023depression} has demonstrated significant promise in capturing emotional signals in individuals with depression. However, these studies are constrained in their applicability to real-world deployments due to computational cost, cost of devices, and user efforts to wear extra devices, particularly when continuous monitoring is essential for delivering timely interventions based on signals captured from users. The resolution of these issues remains unexplored. Therefore, our study aims to bridge these gaps by proposing, collecting, and evaluating the feasibility of deploying data using our novel affective mobile computing system in a real-world, naturalistic setting.


\subsection{Privacy and Ethical Considerations to Enhance Feasibility in Real-World Settings}
Because a camera temporarily captures a user's face on their smartphone for up to 10 seconds before deletion, users may have concerns due to the human perception of 'using a camera,' even though the system does not record any videos. To balance privacy considerations and data quality for modeling, researchers in the HCI community should consider designing user nudging \cite{balebako2014improving, felt2012ve}. These nudging, as suggested by researchers in privacy-preserving systems \cite{denning2014situ}, could allow users to push/pull status about system behavior when running in the background using content that includes appropriate information about data collection status in private contexts where they may feel uncomfortable, even when no images are collected/stored. 

While our system automatically removes images after near-real-time feature extraction, the concept of passive sensing \cite{denning2014situ} that underpins our affective mobile sensing framework does not necessarily require a user interface to reduce users' burden; instead, it runs in the background. This is in contrast to active sensing, typically used for manual tasks. Nevertheless, it is essential to empower users with the ability to enable or disable specific functionalities whenever they perceive potential risks, rather than requiring them to exit the study. While there is a trade-off between privacy and the quantity of data collected, it is crucial for HCI researchers to collaborate in designing mental health tracking systems that enhance user engagement \cite{o2008user}, encourage self-reflection \cite{li2010stage}, motivation \cite{consolvo2006design}, and trust \cite{kelley2009nutrition}, as has been well-established in the field of personal informatics. For example, our individual facial behavior and head pose features can provide users with daily happiness percentage scores using facial expression algorithms. This can serve as one of the motivational strategies that could encourage them to reflect on their mental condition and enable them to maintain good mental health practice, potentially contributing valuable data for long-term healthcare research in the field of mental health. 

To balance privacy considerations and data quality during system design, researchers should consider how data could be collected, used, and stored, as well as what implications this could have for the privacy of the users. While there may be discussion over how data processing can dictate/drive the level of invasiveness of the application when the users are given an option of choosing which type of processing or filters (presenting blurry face images for facial feature extraction) \cite{denning2014situ} they would allow the developers to carry out. At the same time, this necessary allowing the user to do so may impact the number of signals coming from the user's facial data for accurate behavior modeling. 


We highlight the high monetary costs associated with wearable-based physiological markers and the lack of rich emotional data. While we agree with the observation regarding the financial aspect, we would want to adequately address the potential privacy costs of camera-based sensing, which collects user data opportunistically throughout the day. The privacy cost in this context refers to the potential risk of unauthorized access or misuse of personal and sensitive visual data captured by the cameras. A method to reduce this impact could involve discarding the user images immediately after computing the sensing values, thereby minimizing the storage of potentially sensitive information or performing the data processing in memory. Further, we would reduce the time for automatic feature extraction (currently 10 seconds per image). While we also provided notification to users that FacePsy is collecting data in the background, more comprehensive and clear justification that considers monetary and privacy costs and strategies to mitigate potential concerns. Facial data of a person contains various characteristics of the face such as shape (face landmarks, smile probability), eye shape (eye-aspect ratio), muscle movements (Action Units), and face orientation (head Euler angles). While these characteristics describe the state of the face at any given moment but don't reveal a person's identity.  It is important to ensure that any facial data gathered is encrypted and securely stored, as well as ensure that a user's identity is not revealed in any way. A utilitarian approach from the normative ethics point of view \cite{ruotsalainen2020health} to privacy-maintaining interactions with such data is to balance the benefits that can be gained from facial recognition technology with the potential privacy risks. This approach involves considering the trade-offs between the benefits of using facial recognition technology and the potential risks to privacy. This approach requires making decisions that prioritize the greater good, such as deciding to collect only the minimum amount of data necessary, processing the data in a way that does not reveal user identity, and encrypting and securely storing the data. Additionally, this approach may include taking steps to ensure that facial recognition technology is used responsibly and ethically, such as providing users with information on how their data is being collected and used and allowing them to opt out of the system if they choose to.

\subsubsection{User Feedback}

We collected feedback (questionnaire available in Appendix \ref{A_Study_Feedback})  from participants at the end of the study, providing valuable insights into their experiences and perceptions regarding the FacePsy app. Here, we explore the initial reactions, adjustments to the data collection processes, and the evolving acceptance of privacy measures throughout the study. This feedback is instrumental in understanding the real-world implications of deploying such technology and informs potential enhancements to improve user experience and trust. Below are detailed reflections across five key areas, supported by direct quotes from the participants, illustrating the nuanced reactions and suggestions for future development.

\begin{itemize}
     \item \textbf{Initial Perceptions and Consent}: Participants initially had mixed feelings about facial data collection. For instance, P10 stated, \textit{"I was a little skeptical my apps might hung due to this new functionality."} Despite initial hesitations, the consent process was generally found to be reassuring. P8 shared, \textit{"Yes, everything was properly addressed and I was well informed,"} reflecting a sentiment that the privacy and data usage concerns were adequately managed. Some participants (P23, P30) showed an initial discomfort with the app collecting their sensitive facial data regarding privacy. This discomfort may arise due to the introduction of new technology which they haven't used previously. While discomfort was mentioned by participants, they showed a shift in their comfort level using FacePsy over a period of use. 

    \item \textbf{Experience with Data Collection Triggers}: The experience of the app activating on phone unlocking or opening trigger apps varied. While some participants adjusted over time, others remained concerned. P35 commented, \textit{"It definitely became more acceptable over time. Even though I wasn't particularly concerned about data collection, I got more used to it after the first few days. "} showing a quick adaptation, while P24 noted, \textit{"It was a bit of an adjustment but became normal with time"}, showing slow adoption of apps trigger data collection in their daily smartphone use. Whereas P8 noted, \textit{"It was a slight concern at the start but I got used to it eventually."}

    \item \textbf{Impact on Daily Use and Privacy}: Changes in usage habits due to the app’s data collection were minimal. The feature that automatically discarded images after 20 seconds was positively received. For instance, P24 appreciated this feature, saying, \textit{"Increased comfort level"}, P23 mentioned \textit{"A bit of comfort, but I am also aware that social media companies convert the images into metadata. where the original image is no longer needed"}. This shows participants were educated about such data collection methods. While P35 mentioned, \textit{"It may have subconsciously led me to use my phone less frequently at beginning. But I got used to over time and got back to my normal phone usage habits. It didn't make me avoid using any specific apps."}

    \item \textbf{Understanding and Trust on-device Feature Extraction}: Understanding of how facial features were extracted varied, with some participants expressing a desire for more information. Trust was generally high for those who felt adequately informed. For example, P30 affirmed, \textit{"I trust it since it was made clear by the researchers that this was the case. I think as far as trust at the data collection, this is a great approach. As long as the image data is not leaving my phone, I do not have any issues with it."}. P35 showed increased comfort level with using their phone over time, noting \textit{"This was the main reason why I became more and more comfortable using my phone after the initial period. I think this was the key feature that made me stick with the study until the end."}. P23 advocated for a mechanism to create a private repository, noting \textit{"Create a private repository and also alert when the data is accessed with providing the reason and by whom it was accessed"} -- regarding the ownership and control of their personal data.

    \item \textbf{Long-term Acceptance}: Perceptions of the app improved over time for many participants. Regarding suggested improvements, P8 recommended, \textit{"I would make the activation not so random but in timed intervals"} calling for enhanced user control over the data collection processes. A similar sentiment was echoed by P35, \textit{"Giving users personalization options for which apps to exclude for data collection would be beneficial. Some users may prefer excluding certain apps from being monitored, and I think having this option would increase user acceptance."}
\end{itemize}

\subsection{Potential to Integrate Affective and Cognitive Inferences From Facial Behavior Markers with Conversational Artificial Intelligence (CAI)}
As previous research has confirm that understanding facial behavior primitives can enhance long-term solutions for managing depression and provide insights into emotional communication \cite{hammal2014intra}, aggression and negative affect recognition \cite{fitrianie2023head}, psychological distress \cite{stratou2017multisense}, and automatic thoughts perception during cognitive behavioral therapy (CBT) \cite{shidara2022automatic, shidara2020analysis}, inferences drawn from facial behavior primitives in our study could enable more effective affective interactions \cite{conati2005affective, islam2023revolutionizing} with a virtual CBT agent \cite{shidara2022automatic, shidara2020analysis}, depending on an individual’s emotional and cognitive state. Such an approach aims to create more affect-sensitive multimodal human–computer interactions \cite{pantic2003toward}, enhancing the human-like qualities, effectiveness, and efficiency of virtual agents. Real-time interpretation of complex mental states from facial expressions and head gestures, indicating concentration, fatigue, disagreement, interest, contemplation, uncertainty, and more \cite{el2005real}, could provide contextual affect data to virtual agents, facilitating more fluid interactions. This aspect is often lacking in text-based and avatar-based therapy agents.

In addition to its potential in depression detection, facial behavior primitives could find application in real-time intoxication detection, as demonstrated in previous work on drunk face detection using an offline dataset \cite{mehta2019dif}. By incorporating such capabilities, the FacePsy system could broaden its utility and use cases for real-world applications. Moreover, the system's applicability could extend to the detection of other forms of intoxication, such as marijuana intoxication \cite{bae2021mobile, chung2020mobile}, where observable facial muscle and head movements are indicative of psychomotor retardation and agitation. Recent developments, like the creation of an augmented reality feedback system for facial paralysis in a mobile setting \cite{barrios2021farapy}, hint at the potential utility of the FacePsy system in similar deployments. Beyond health-related applications, the FacePsy system's usefulness could be explored in monitoring student engagement in smartphone-based education or virtual classes \cite{islam2023microflow} where instructors can adjust lecture materials based on students' behavioral feedback. Indicators such as concentration, interest, or uncertainty are crucial for enhancing classroom engagement and delivering high-quality education. The FacePsy system represents a significant step in the realm of mobile affect sensing in human-computer interaction, with far-reaching implications for mental health with virtual therapy sessions, substance abuse detection, and educational engagement. 