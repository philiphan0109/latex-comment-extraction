\documentclass[acmsmall,screen]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}

% %% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[MobileHCI '24]{MobileHCI '24: ACM MobileHCI}{Sept 30 -- Oct 03, 2024}{Melbourne, Australia}
\acmBooktitle{Proceedings of the ACM on Human-Computer Interaction: ACM MobileHCI,
  Sept 30 -- Oct 03, 2024, Melbourne, Australia}
% \acmPrice{15.00}
\acmISBN{}




\usepackage{array}
\usepackage[section]{placeins}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{FacePsy: An Open-Source Affective Mobile Sensing System -- Analyzing Facial Behavior and Head Gesture for Depression Detection in Naturalistic Settings
}

% A Pilot Study on Facial Behavior and Head Gesture Analysis for Depression Detection in Naturalistic Settings} 
%Using Passively Collected Facial Behavior Data}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Rahul Islam}
% \authornote{Both authors contributed equally to this research.}
\email{mislam5@stevens.edu}
\orcid{0000-0003-3601-0078}
\author{Sang Won Bae}
\authornote{Corresponding author.}
\orcid{0000-0002-2047-1358}
\email{sbae4@stevens.edu}
\affiliation{%
  % \institution{Stevens Institute of Technology}
  \institution{Charles V. Schaefer, Jr. School of Engineering and Science, Stevens Institute of Technology}
  \streetaddress{1 Castle Point Terrace}
  \city{Hoboken}
  \state{New Jersey}
  \country{USA}
  \postcode{07030}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Islam and Bae}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Depression, a prevalent and complex mental health issue affecting  millions worldwide, presents significant challenges for detection and monitoring. While facial expressions have shown promise in laboratory settings for identifying depression, their potential in real-world applications remains largely unexplored due to the difficulties in developing efficient mobile systems. In this study, we aim to introduce FacePsy, an open-source mobile sensing system designed to capture affective inferences by analyzing sophisticated features and generating real-time data on facial behavior landmarks, eye movements, and head gestures -- all within the naturalistic context of smartphone usage with 25 participants. Through rigorous development, testing, and optimization, we identified eye-open states, head gestures, smile expressions, and specific Action Units (2, 6, 7, 12, 15, and 17) as significant indicators of depressive episodes (AUROC=81\%). Our regression model predicting PHQ-9 scores achieved moderate accuracy, with a Mean Absolute Error of 3.08. Our findings offer valuable insights and implications for enhancing deployable and usable mobile affective sensing systems, ultimately improving mental health monitoring, prediction, and just-in-time adaptive interventions for researchers and developers in healthcare.

% Depression is a complex mental health condition affecting millions worldwide. While facial actions have shown promise in lab settings for understanding depression, their application in real-world scenarios remains largely unexplored due to challenges in designing efficient, deployable mobile systems. To bridge this gap, we introduce FacePsy, an open-source mobile sensing system capturing facial features, generating real-time data on facial behavior landmarks, eye movements, and head gestures, all through smartphones in natural settings, while preserving user privacy. Our research identifies specific head gestures and lip corner depressors as key indicators of depressive episodes. These findings have significant implications for designing improved mobile affective systems for mental health monitoring, prediction, and timely interventions.
\end{abstract}




%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003138.10003142</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing design and evaluation methods</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing design and evaluation methods}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Affective computing, Depression, Machine Learning, Mobile computing, System, Empirical study that tells us about people, Application Instrumentation, Field Study}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{27 August 2023}
% % \received[revised]{12 March 2009}
% \received[accepted]{1 September 2023}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\input{Section/1_Introduction}
\input{Section/2_Related_Work}
\input{Section/3_Method}
\input{Section/4_Results}
% \input{Section/5_Model_Comparison}
\input{Section/6_Discussion}
\input{Section/7_Limitation}
%\input{Section/8_Future_Work}
\input{Section/9_Conclusion}

\section{Acknowledgments}
We sincerely thank our research volunteer, Shahnaj Laila, for her kind assistance in conducting the feasibility study. We are also grateful to the
participants who generously agreed to share their facial behavior data for
this study.

\bibliographystyle{ACM-Reference-Format}
\bibliography{Reference}

\input{Section/10_Appendix} 

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
