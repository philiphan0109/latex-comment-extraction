\section{Hyper-parameters considered} 
\label{sec:hyper-parameter_selection}

We describe the set of hyper-parameters explored in this work, with the values used for each listed in \autoref{sec:list_hyperparameters}. Unless otherwise specified, these are examined for both Conv and Dense layers.

{\bf Activation functions:} 
Non-linear activation functions are a fundamental part of neural networks, as their removal effectively turns the network into a linear function approximator.
While various activation functions have been proposed \citep{devlin2019bert, Elfwing2018SigmoidWeightedLU, 10.5555/3305381.3305478}, there have been few works comparing their performance \citep{Shamir2020SmoothAA}; to the best of our knowledge, there are no previous examples of such a comparison in the RL setting.


{\bf Normalization: }
Normalization plays an important role in supervised learning \citep{tan2020efficientnet, xie2017aggregated} but is relatively rare in deep reinforcement learning, with a few exceptions \citep{gogianu2021spectral, bhatt2019crossnorm, arpit2019initialize, alphaZero}. We explore {\em batch normalization} \citep{ioffe2015batch} and {\em layer normalization} \citep{ba2016layer}.

{\bf Network capacity: } 
``Scaling laws'' have been central to the growth of capabilities in large language/vision models, but have mostly eluded reinforcement learning agents, with a few exceptions \citep{schwarzer23a, taiga2022investigating, farebrother2022proto,obando2024mixtures,obandoceron2024pruned,farebrother2024stop}. 
To investigate the impact of network size, we vary the {\em depth} (e.g. the number of hidden layers) and the {\em width} (e.g. the number of neurons of each hidden layer).

{\bf Optimizer hyper-parameters: }
\label{sec:optimizerHypers}
We explore three hyper-parameters of Adam \citep{kingma15adam}, which has become the standard optimizer used by most: {\em learning rate}, {\em epsilon} and {\em weight decay}.
\emph{Learning rate} determines the step size at which the algorithm adjusts the model's parameters during each iteration.
$\epsilon$ represents a small constant value that is added to the denominator of the update rule to avoid numerical instabilities.
\emph{Weight decay} adds a penalty term to the loss function during training that discourages the model from assigning excessively increasing weight magnitudes.


{\bf $\epsilon$-greedy exploration: } 
$\epsilon$-greedy exploration is a simple and popular exploration technique which picks actions greedily with probability $1-\epsilon$, and a random action with probability $\epsilon$. Traditionally, experiments on the ALE use a linear decay strategy to decay $\epsilon$ from $1.0$ to its target value.

{\bf Reward clipping: } 
Most ALE experiments clip rewards at $(-1, 1)$ \citep{mnih2015humanlevel}.

{\bf Discount factor: } 
The multiplicative factor $\gamma$ discounts future rewards and its importance has been observed in a number of recent works \citep{amit2020discount, hessel19inductive, gelada2019off, vanseijen2019using, francoislavet2016discount,schwarzer23a}.

{\bf Replay buffer: }  
DRL agents  store past experiences in a replay buffer, to sample from during learning. The {\em replay capacity} parameter refers to the amount of data experiences stored in the buffer. 
It is common practice to only begin sampling from the replay buffer when a minimum number of transitions have been stored, referred to as the {\em minimum replay history}.


{\bf Batch size: } 
The number of stored transitions that are sampled for learning at each training step.

{\bf Update horizon: }
Multi-step learning \citep{sutton88learning} computes the temporal difference error using multi-step transitions, instead of a single step. DQN uses a single-step update by default, whereas Rainbow chose a 3-step update \citep{Hessel2018RainbowCI}. The update horizon has been argued to trade-off between the bias and the variance of the return estimate \citep{biasandvariance_kea}. 


{\bf Target Update periods: }
Value based agents often employ an online and a {\em target} Q-network, the latter which is updated less frequently by directly syncing (or Polyak-averaging) from the online network; the {\em target updated period} determines how frequently this occurs.


{\bf Update periods: }
The online network parameters are updated after every {\em update period} environment steps, with a value of $4$ used in standard ALE training.

{\bf Number of atoms: } 
In distributional reinforcement learning \citep{Bellemare2017ADP}, the output layer predicts the distribution of the returns for each action $a$ in a state $s$, instead of the mean $Q^{\pi}(s, a)$. A popular approach is to model the return as a categorical distribution parameterized by a certain number of 'atoms' over a pre-specified support. 


\begin{figure}[!t]
    \centering
  \includegraphics[width=\linewidth]{figures/this_score_all.pdf}%
    \caption{Tuning hyper-parameter Consistency (THC Score, see \cref{sec:thc_metric}) evaluated across agents (\textbf{left panel}), data regimes (\textbf{center panel}), and environments  (\textbf{right panel}). Different colors indicate different data regimes (left panel) and different agents (center and right panels); grey bars/titles indicate hyper-parameters which are not comparable across the considered transfer settings.
    \label{fig:this_score_all}%
    }%
\end{figure}



\section{Experimental results} 
\label{exp_results}
As mentioned in the introduction, there already exist two data regimes for evaluating agents on the ALE suite: the (low-data regime) $100$k \citep{kaiser2020modelbased} and the original $200$M benchmark \citep{mnih2015humanlevel}. The $100$k benchmark includes only $26$ games from the original suite, so we focus on these for our evaluation. For computational considerations, we follow \citet{graesser2022state} and use $40$M million environment frames as our large-data regime.
We use the settings of DrQ($\epsilon$) (introduced by \citet{agarwal2021deep} as an improvement over the DrQ of \citet{yarats2021image}), and 
Data Efficient Rainbow (DER) introduced by \citet{hasselt19when}. All experiments were run on a Tesla P100 GPU and took around $2$-$4$ hours ($100$k) and $1$-$2$ days ($40$M) per run.
Both algorithms are implemented in the Dopamine library \citep{castro18dopamine}. Since the $100$k setting is cheaper, we evaluated a larger set of hyper-parameter values there and manually picked the most informative subset for running in the $40$M setting. For all our experiments we ran 5 independent seeds and followed the guidelines suggested by \citet{agarwal2021deep} for more statistically meaningful comparisons. Specifically, we computed aggregate human-normalized scores and report interquantile mean (IQM) with $95\%$ stratified bootstrap CIs. 

In \autoref{fig:this_score_all} we present the computed THC score for all the hyper-parameters discussed in \cref{sec:hyper-parameter_selection}, and we discuss their consistency across agents in Section~\ref{sec:acrossAlgorithms}, across data regimes in Section~\ref{sec:acrossData}, and  across environments in Section~\ref{sec:acrossEnvironments}. More detailed discussions are provided in \autoref{sec:finerGrainedExperiments} and a set of interesting findings in \autoref{sec:imf}. It is worth recalling that higher THC scores indicate less consistency, which suggests a likely need to re-tune the respective hyper-parameters when changing training configurations.


\subsection{Optimal hyper-parameters mostly Transfer Across Agents}
\label{sec:acrossAlgorithms}
We find that optimal hyper-parameters for DrQ($\epsilon$) agree quite often with DER, which is somewhat expected given that they're based on the same classical RL algorithm of Q-learning, and have the same number of updates in the same environments. Looking at THC values between the two agents for different data regimes we see that all values are below $0.5$, and in the $100$k regime tend to be even lower. Nevertheless, comparing the results of the two rows in \cref{fig:drq_eps_batch_sizes,fig:per_game} demonstrate that there can still be strong differences between the two. In the $40$M regime, the hyper-parameters with the highest THC are batch size and update horizon, consistent with the findings of \cite{obandoceron2023small}, where these two hyper-parameters proved crucial to boosting agent performance.


\begin{figure}[!t]
    \centering
  \includegraphics[width=0.8\linewidth]{figures/DER_adam_eps.pdf}%
    \caption{
     \textbf{Measured IQM of human-normalized scores on the $26$ $100$k benchmark games, with varying Adam's $\epsilon$} for DER. We evaluate performance at 100k agent steps (or 400k environment frames), and at $40$ million environment frames. The ordering of the best hyper-parameters switches between the two data regimes.
    }
    \label{fig:der_adam_eps}
\end{figure}

\subsection{Optimal hyper-parameters mostly do not Transfer Across Data Regimes}
\label{sec:acrossData}
We find that optimal hyper-parameters for Atari 100k mostly do not transfer once you move to 40M updates, showing that even when keeping algorithms and environment constant one may still need to tune hyper-parameters should they change the amount of data their agent can train on. Of the hyper-parameters considered, {\em Adam's $\epsilon$} and {\em update period} seem to be the most critical to re-tune (see \autoref{fig:der_adam_eps} for results on DER for Adam's $\epsilon$). The results with Adam's $\epsilon$ are surprising, as the purpose of this hyper-parameter is mostly for numerical stability. The update horizon results are consistent with what is done in practice between these two data regimes (e.g. Rainbow uses an update horizon of $3$, while DER uses $10$).

\begin{figure}[!h]
    \centering
  \includegraphics[width=0.8\linewidth]{figures/DrQ_eps_subs.pdf}
  \includegraphics[width=0.8\linewidth]{figures/DER_subs.pdf}
    \caption{\textbf{Measured returns with varying batch size} for DrQ($\epsilon$) (top) and DER (bottom) at $40$M environment frames for four representative games, demonstrating that the ranking of the hyper-parameter values can drastically change from one game to the next. All results averaged over $5$ seeds, shaded areas represent $95\%$ confidence intervals.
    }%
    \label{fig:drq_eps_batch_sizes}%
\end{figure}


\subsection{Optimal hyper-parameters do not Transfer Across Environments}
\label{sec:acrossEnvironments}
Our experiments show that hyper-parameters that perform well on some games lead to lackluster final performance in others. Indeed, in \autoref{fig:this_score_all} we can see that the THC score is highest when evaluating across environments. This strongly suggests that, when using an existing agent in a new environment, most of the hyper-parameters would need extra tuning.
\autoref{fig:drq_eps_batch_sizes} displays the results when varying batch size, where we can see that the rankings can sometimes be complete opposites across games (compare Kangaroo and Gopher).
 


\section{A web-based appendix} 
\label{web_results}
We have run an extensive number of experiments (around 108k) for this work, which would render a traditional appendix unwieldy. Instead, we provide an interactive website\footnote{Website available at \href{https://consistent-hyperparameters.streamlit.app/}{\emph{https://consistent-hparams.streamlit.app/}}.} which facilitates navigating the full set of results. Presenting empirical research results in this manner offers a range of benefits that enhance accessibility, engagement, and comprehension. 
This dynamic presentation allows readers to more easily make comparisons over different games, agents, and parameters. 


The website's main page presents aggregate IQM results for all hyper-parameters investigated in both data regimes (e.g. \autoref{fig:der_adam_eps}), while sub-pages present detailed performance comparisons when sliced by game (\autoref{fig:drq_eps_batch_sizes} presents a subset of this) and hyper-parameter (\autoref{fig:per_game} presents a subset of this).
The added level of granularity provided by the sub-pages can be crucial for understanding the specific strengths and weaknesses of an algorithm in various scenarios. All results averaged over 5 seeds, shaded areas represent 95\% confidence intervals.

\begin{figure}[!t]
    \centering
   \includegraphics[width=\textwidth]{figures/DrQ_eps_game_subs.pdf}
   \includegraphics[width=\textwidth]{figures/DER_game_subs.pdf}
  
    \caption{\textbf{Measured returns with various hyper-parameter variations on Asterix} for DrQ($\epsilon$) (top) and DER (bottom) at 40M environment frames. Displaying eight representative hyper-parameters, enabling per-game analyses for hyper-parameter selection.}%
    \label{fig:per_game}%
    \vspace{-1em}
\end{figure}