\section{Discussion}
\label{sec:discussion}
One of the central challenges in reinforcement learning research is the non-stationarity during training in the inputs (due to self-collected data) and targets (due to bootstrapping). This is in direct contrast with supervised learning settings, where datasets and labels are typically fixed throughout training. This non-stationarity may be largely to blame for some of the ranking inconsistencies observed under different training regimes (e.g. \autoref{fig:der_adam_eps}), and why different hyper-parameter tunings are required for different settings (e.g. DER versus Rainbow).

Hyper-parameters are commonly tuned on a subset of environments (e.g. 3-5 games) and then evaluated on the full suite. Our findings suggest that this approach may not be the most rigorous, as hyper-parameter selection can vary dramatically from one game to the next (c.f. \cref{fig:drq_eps_batch_sizes,fig:per_game}).
While aggregate results (e.g. IQM) provide a succinct summary of performance, they unfortunately gloss over substantial differences in the individual environments. If our hope as researchers is to be able to use these algorithms beyond academic benchmarks, understanding these differences is {\em essential}, in particular in real-world applications such as healthcare and autonomous driving.

We have conducted a large number of experiments to investigate the impact of various hyper-parameter choices. While the THC score (\autoref{fig:this_score_all}) provides a high-level view of the transferability of hyper-parameter choices, our collective results suggest that a {\em single} set of hyper-parameter choices will never suffice to achieve strong performance across all environments. The ability to dynamically adjust hyper-parameter values during training is one way to address this; to properly do so would require quantifiable measures of environment characteristics that go beyond coarse specifications (such as sparse versus dense reward systems). The per-game results we present here may serve as an initial step in this direction. In Appendix~\ref{sec:gopher} we provide a fine-grained analysis of DER on Gopher as an example of the type of analyses enabled by our website. We hope our analyses, results, and website prove useful to RL researchers in developing robust and  transferable algorithms to handle increasingly complex problems.\\

\subsubsection*{Acknowledgements}

The authors would like to thank Jesse Farebrother, Gopeshh Subbaraj, Doina Precup, Hugo Larochelle, and the rest of the Google DeepMind Montreal team for valuable discussions during the preparation of this work.  Jesse Farebrother deserves a special mention for providing us valuable feed-back on an early draft of the paper. We thank the anonymous reviewers for their valuable help in improving our manuscript. We would also like to thank the Python community \cite{van1995python, 4160250} for developing tools that enabled this work, including NumPy \cite{harris2020array}, Matplotlib \cite{hunter2007matplotlib}, Jupyter \cite{2016ppap}, Pandas \cite{McKinney2013Python} and JAX \cite{bradbury2018jax}.

\subsubsection*{Broader Impact Statement}

Although the work presented here is mostly academic, it aids in the development of more capable and reliable autonomous agents. While our contributions do not directly contribute to any negative societal impacts, we urge the community to consider these when building on our research.