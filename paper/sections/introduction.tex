\section{Introduction}
\label{sec:introduction}

Sequential decision making is generally considered an essential ingredient for generally capable agents. The ability to plan ahead and adapt to changing circumstances is synonymous with the concept of {\em agency}. For decades, the field of reinforcement learning (RL) has worked on developing methods, or agents, for precisely this purpose. This research has borne impressive results, such as developing agents which can play difficult Atari games \citep{mnih2015humanlevel}, control stratospheric balloons \citep{Bellemare2020AutonomousNO}, control a tokamak fusion reactor \citep{Degrave2022MagneticCO}, among others. These are all examples of {\em deep reinforcement learning} (DRL), which combines the theory of reinforcement learning with the expressiveness and flexibility of deep neural networks.

The success of these methods built on years of academic research, where novel algorithms and techniques were introduced and showcased on academic benchmarks such as the ALE \citep{bellemare2012ale}, MuJoCo \citep{todorov2012mujoco}, and others. These benchmarks typically consist of a suite of environments that have varied transition and reward dynamics. Their common usage provides us with a familiarity which affords us a sense of interpretability, a consistency in evaluation that grants us a sense of reliability, and their variety yields a sense of generalizability. Unfortunately, this promise often fails to materialize: their reliability has been brought into question by numerous works which demonstrate their fickleness \citep{Henderson2017DeepRL,agarwal2021deep}, while there is a general sentiment that researchers have ``overfit’’ to these benchmarks, bringing into question their generalizability. A critical aspect to these challenges is the difficulty in training neural networks in an RL setting \citep{ostrovski2021the,lyle2022learning,sokar2023dormant}.

Although the successes above built on prior methods, they were not taken ``as is’’: it took large teams of researchers many months and lots of compute to adapt prior work to their specific problem. These adaptations include changes to the network architectures, designing reward functions to induce the desired behaviours, and careful tuning of the many hyper-parameters. This last point is indeed {\em essential} to the success of any DRL method: improper hyper-parameter choices can cause a theoretically sound method to drastically underperform, while careful hyper-parameter selection can dramatically increase the performance of an otherwise sub-optimal method.

As an example of this dichotomy, we examine how DER \citep{hasselt19when}, a method that has become a common baseline for the Atari $100$k benchmark \citep{kaiser2020modelbased}, came to be. DQN, considered to be the start of the field of DRL research, was introduced by showcasing its super-human performance on the ALE \citep{bellemare2012ale}, a suite of $57$ Atari $2600$ games. This suite became one of the most popular benchmarks on which to evaluate new methods over $200$ million environment frames\footnote{See \citep{machado2018revisiting} for more details on ALE evaluation standards.}. A few years later, when \citet{kaiser2020modelbased} introduced the SiMPLe algorithm as a sample-efficient method, they argued for evaluating it only on $100$k agent actions\footnote{The standard for ALE agents is to use frame-skipping, where $4$ environment frames occur for every agent action. This results in frustratingly confusing nomenclature, as $200$M is specified in environment frames (or $500$k agent actions), while $100$k is specified in agent actions (or $400$k environment frames).} with a subset of $26$ games, so as to properly test the sample-efficiency of new methods. The authors demonstrated that their proposed method outperformed Rainbow \citep{Hessel2018RainbowCI}, the state-of-the-art method of the time. In response, \citet{hasselt19when} introduced Data Efficient Rainbow (DER), which outperformed SiMPLe even though it was the same Rainbow algorithm, but {\em with a careful tuning of the hyper-parameters for the $100$k training regime}.

One could argue that the hyper-parameters of Rainbow were overly-tuned to the $200$M benchmark, while the hyper-parameters of DER were overly-tuned to the $100$k benchmark. More importantly, what this story highlights is that, despite careful evaluation it is quite likely that a new method {\em will not work as intended when deployed on a different environment from which it was trained on}, and that a significant  amount of hyper-parameter tuning will be necessary. This flies in the face of the supposed generalizability of DRL academic research, and makes it difficult for groups without large computational budgets to successfully apply prior work to applied problems.

It thus behooves the community to develop a better understanding of the {\em transferability} and {\em consistency} of hyper-parameter selection across different training regimes, and to build a better shared understanding of the relative importance of the many possible hyper-parameters to tune. In this work, we take a stride towards this by conducting an exhaustive empirical investigation of the various hyper-parameters affecting DRL agents. We focus our attention on two value-based agents developed for the Atari $100$k suite: DER mentioned above, and DrQ($\epsilon$), a variant of DQN that was optimized for the $100$k suite. Although developed for the $100$k suite, we also train these agents for $40$M million environment frames. Our intent is to examine the transferability of various hyper-parameter choices across different training regimes. Specifically, we investigate:
{\bf Across data regimes:} Do hyper-parameters selected in the $100$k regime work well in a larger data regime? {\bf Across agents:} Do hyper-parameters selected for one agent work well in another? {\bf Across environments:} Do hyper-parameters tuned in one set of environments work well in others?

In total, we investigated $12$ hyper-parameters with different values for $2$ agents over $26$ environments, each for $5$ seeds, resulting in a total of $108$k independent training runs. This breadth of experimentation results in an overwhelming amount of data which complicates their analyses. We address this challenge in two ways: \textit{(i)} We introduce a new score which provides us with an aggregate value for the considerations mentioned above. \textit{(ii)} We provide an interactive website where others may easily navigate the large number of experimental figures we have generated.

The score provides us with a high-level overview of our findings, while the website grants us a fine-grained mechanism to analyze the results. We hope this effort provides the community with useful tools so as to develop not just better DRL algorithms, but better methodologies to evaluate their interpretability, reliability, and generalizability.