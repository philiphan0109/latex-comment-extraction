\section{Computing Tuning Hyperparameter Consistency (THC)}
\label{sec:appendixTHC}
Computing the ranking between hyper-parameter values is non-trivial given the noise involved in Deep Reinforcement Learning Agents performances. We used 5 seeds to improve the robustness to noise of our results in this paper, but if we simply used the average performance the effects of noise would still be significant. As such our ranking is based on the Inter-Quantile Mean (IQM) \citep{agarwal2021deep} and its 95\% confidence interval.

First we sort the performance array in decreasing order based on the upper bound of the confidence interval for each hyper-parameter. Then we compute the rank of each hyper-parameter as the average between the lowest position (1-based) whose lower bound is less than or equal to the current hyper-parameter's performance upper bound and the highest position whose upper bound is greater than or equal to the current hyper-parameter's lower bound. Our choice of treating overlaps in performance by averaging the rankings comes from what is typically done when dealing with ties when computing Kendall's W and Kendall's $\tau$, which are other commonly used metrics for inter-ranking agreement.

As an example imagine we are analyzing a hyper-parameter with 5 possible values, 1e-2, 1e-1, 1, 1e1, 1e2. We run all the experiments and get the following confidence intervals on their IQM ranges $(200, 300), (250, 350), (400, 600), (110, 220), (30, 70)$. 
After sorting them we're left with: 
\begin{enumerate}
    \item 1: (400, 600)
    \item $10^{-1}$: (250, 350)
    \item $10^{-2}$: (200, 300)
    \item $10^1$: (110, 220)
    \item $10^2$: (30, 70)
\end{enumerate}
Then we can compute the ranks as: 
\begin{enumerate}
    \item $1: \frac{1 + 1}{2} = 1$
    \item $10^{-1}: \frac{2 + 3}{2} = 2.5$
    \item $10^{-2}: \frac{2 + 4}{2} = 3$
    \item $10^1: \frac{3 + 4}{2} = 3.5$
    \item $10^2: \frac{5 + 5}{2} = 5$
\end{enumerate}
An important feature of this method is that ranks needs not be integers. Now another relevant example is one where the 3 values, let's call them A, B, C, have completely overlapping intervals: \begin{enumerate}
    \item A: (200, 300)
    \item B: (250, 350)
    \item C: (180, 260)
\end{enumerate}
In this case all of them will have the ranking $\frac{1+3}{2} = 2$, which shows how given our results we're unable to fully determine which one is the best or worst performing value for this hyper-parameter. 

Here are two extra examples of computing the THC score.

\begin{enumerate}
    \item We analyze a case with 2 hyper-parameters, $A_1$ and $B_1$, both with 3 values, being evaluated across 5 games (columns are ranks in a game):

    \begin{equation*}
    \begin{aligned}
        r_{A_1} &= \begin{bmatrix}
                    1 & 1 & 2 & 1 & 3\\
                    2 & 3 & 2 & 3 & 2\\
                    3 & 2 & 2 & 2 & 1
                    \end{bmatrix} \\
        r_{B_1} &= \begin{bmatrix}
                    1 & 2 & 1 & 2 & 1\\
                    2 & 1 & 2 & 1 & 2\\
                    3 & 3 & 3 & 3 & 3
                    \end{bmatrix}
    \end{aligned}
    \xrightarrow{\text{peak-to-peak}}
    \begin{aligned}
        ptp_{A_1} &= \begin{bmatrix}
                    2\\
                    1\\
                    2
                    \end{bmatrix} \\
        ptp_{B_1} &= \begin{bmatrix}
                    1\\
                    1\\
                    0
                    \end{bmatrix}
    \end{aligned}
    \xrightarrow{\text{Normalize}}
    \begin{aligned}
        ptp_{A_1} &= \begin{bmatrix}
                    1.0\\
                    0.5\\
                    1.0
                    \end{bmatrix} \\
        ptp_{B_1} &= \begin{bmatrix}
                    0.5\\
                    0.5\\
                    0.0
                    \end{bmatrix}
    \end{aligned}
    \end{equation*}
    Finally we average the values to get the THC for each hyper-parameter:
        \begin{align}
            THC_{A_1} &= \frac{2.5}{3} \approx 0.83333  & THC_{B_1} &= \frac{1.0}{3} \approx 0.33333
        \end{align}

    
    This example also shows an important property of THC, while $a_1$ seems to be consistently the best value for A, whereas $b_1$ and $b_2$ vary their position more often, the value of THC is higher for A then for B, since the largest change in performance for values of A is larger than the change for values of B. This is because THC considers the worst-case variance when assigning how important is tuning a given hyper-parameter.
    
    \item Another example, now one hyper-parameter, $A_2$, has 4 possible values and the other, $B_2$, has 3, and we have 4 games.
    
\begin{equation*}
    \begin{aligned}
        r_{A_2} &= \begin{bmatrix}
                    1 & 1 & 1 & 3\\
                    2 & 2 & 2 & 2\\
                    3 & 3 & 3 & 1 \\
                    4 & 4 & 4 & 4
                    \end{bmatrix} \\
        r_{B_2} &= \begin{bmatrix}
                    1 & 1 & 1 & 1\\
                    2.5 & 2 & 3 & 2\\
                    2.5 & 3 & 2 & 3
                    \end{bmatrix}
    \end{aligned}
\xrightarrow{\text{peak-to-peak}}
\begin{aligned}
        ptp_{A_2} &= \begin{bmatrix}
                    2 \\
                    0 \\
                    2 \\
                    0
                    \end{bmatrix} \\
        ptp_{B_2} &= \begin{bmatrix}
                    0 \\
                    1 \\
                    1
                    \end{bmatrix}
    \end{aligned}
\xrightarrow{\text{Normalize}}
\begin{aligned}
        ptp_{A_2} &= \begin{bmatrix}
                    \frac{2}{3} \\
                    0.0 \\
                    \frac{2}{3} \\
                    0.0
                    \end{bmatrix} \\
        ptp_{B_2} &= \begin{bmatrix}
                    0.0\\
                    0.5\\
                    0.5
                    \end{bmatrix}
    \end{aligned}
\end{equation*}

    And then average across the hyper-parameter values:
        \begin{align}
            THC_{A_2} &= \frac{\frac{4}{3}}{4} = \frac{1}{3} & THC_{B_2} &= \frac{1}{3}
        \end{align}
    In this case we see that while $A_2$ has 2 hyper-parameter values with more variance in ranking then the 2 values of $B_2$ the fact that $A_2$ has more values overall than $B_2$ leads them to having the same THC value.
\end{enumerate}

Finally it's worth pointing out that since the performances in the second case were more stable than in the first one their THC value was overall lower. 

\section{Finer-grained experimental discussion}
\label{sec:finerGrainedExperiments}

\subsection{Optimal hyper-parameters do not Transfer Across Environments}
\begin{enumerate}
    \item For batch size in DrQ($\epsilon$)@40M we find that 4 is the optimal batch size for Asterix, Breakout, Gopher, and Seaquest, while being the worst value for effectively all the other games. See Figure \ref{fig:drq_eps_batch_sizes}
    \item Convolutional width for DER@40M, 0.25 is the clear optimum in Assault, CrazyClimber, Roadrunner, Seaquest, and UpNDown, while leading to the worst performance in Breakout, Krull, and QBert
    \item Dense layer width for DrQ($\epsilon$)@40M we see that 768 neurons per layer lead to best performance for Amidar, Assault, Hero, and Qbert, while most other games have 128 neurons as their optimal layer width. We see a similar mismatch for DER, though the games were 768 is optimal are different.
    \item A discount factor of 0.99 is optimal for DER@40M in Alien, Amidar, Asterix, BankHeist, Breakout, Frostbite, Kangaroo, Kung Fu Master, QBert, RoadRunner, Seaquest, and UpNDown, but leads to pessimal performance in PrivateEye and non-optmimum in Assault, Boxing, ChopperCommand, CrazyClimber and many others.
\end{enumerate}

\subsection{Optimal hyper-parameters do not Transfer Across Data Regimes}

\begin{enumerate}
    \item Adam's $\epsilon$, an often overlooked hyper-parameter, has optimal values < $1.5 \cdot 10^{-4}$ for Atari 100k, while having optimal value of $1.5 \cdot 10^{-2}$ in the 40M setting. This result also begs for further research, as higher values of $\epsilon$ move Adam closer to SGD with momentum behaviour.
    \item For Convolutional Width we find that the worst performing value for 100k, 0.25, is the optimal value when number of updates is 40M. Another important result given that it means one may want to effectively change the network architecture when the number of updates changes.
    \item For normalization of the dense layers we see that while in the 100k regime Layer Norm leads to worse performance than no normalization, it is the best performing normalization once we move to the 40M regime.
    \item For update horizon one can see that the best performing values are high, around 10, in the 100k regime, while lower values (as low as 1 for DER) are optimal in the 40M regime.
    \item For update period we see that in the 100k regime a value of 6 is low performing and 1 is optimal, but once we move to the 40M regime we see an inversion, where 6 is substantially superior to 1.
\end{enumerate}

\clearpage

\section{Hyper-parameters list}
\label{sec:list_hyperparameters}

Default hyper-parameter settings for DER and DrQ($\epsilon$) across the environments. \autoref{tbl:defaultvalues} shows the default values for each hyper-parameter across all the Atari games. In \autoref{tbl:allvalues} we list all the possible values we explored for both agents. The values selection is informed by the recommendations provided by \citet{joajo2021lifting}.


\begin{table}[!h]
 \centering
  \caption{Default hyper-parameters setting for DER and DrQ($\epsilon$) agents.}
  \label{tbl:defaultvalues}
 \begin{tabular}{@{} ccc @{}}
  %\toprule
    \toprule
    & \multicolumn{2}{c}{Atari}\\
    \cmidrule(lr){2-3}
  Hyper-parameter &  DER & DrQ($\epsilon$) \\
    \midrule
     Adam's($\epsilon$) & 0.00015 & 0.00015\\
     Batch Size & 32 & 32\\
     Conv. Activation Function & ReLU & ReLU \\
     Convolutional Normalization & None & None \\
     Convolutional Width & 1& 1\\
     Dense Activation Function & ReLU & ReLU\\
     Dense Normalization & None & None \\
     Dense Width & 512 & 512 \\
     Discount Factor & 0.99 & 0.99 \\
     Exploration $\epsilon$ & & \\
     Learning Rate & 0.0001 & 0.0001 \\
     Minimum Replay History & & \\
     Number of Atoms & 51 & 0 \\
     Number of Convolutional Layers & & \\
     Number of Dense Layers & 2 & 2\\
     Replay Capacity & 1000000 & 1000000 \\
     Reward Clipping & True & True \\
     Update Horizon & 10 & 10 \\
     Update Period & 1& 1\\
    Weight Decay & 0 & 0\\
     \bottomrule
  \end{tabular}
\end{table}


\begin{table*}[!ht]
 \centering
  \caption{Hyper-parameters settings for DER and DrQ($\epsilon$) agents}
  \label{tbl:allvalues}
 \begin{tabular}{@{} cc @{}}
  %\toprule
    \toprule
    %\cmidrule(lr){1-2}\cmidrule(lr){2-3}
  Hyper-parameter &  Values \\
  \midrule
  Adam's($\epsilon$) & 1, 0.5, 0.3125, 0.03125, 0.003125, 0.0003125, 3.125e-05,\\
  & 3.125e-06 \\
  Batch Size & 4, 8, 16, 32, 64 \\
  Conv. Activation Function &  ReLU, ReLU6, Sigmoid, Softplus, Soft sign, SiLU, \\
  & Log Sigmoid, Hard Sigmoid, Hard SiLU, Hard tanh, ELU, \\
  & CELU, SELU, GELU, GLU \\
  Convolutional Normalization &  None, BatchNorm, LayerNorm \\
  Convolutional Width & 0.25, 0.5, 1, 2, 4 \\
  Dense Activation Function &  ReLU, ReLU6, Sigmoid, Softplus, Soft sign, SiLU, \\
  & Log Sigmoid, Hard Sigmoid, Hard SiLU, Hard tanh, ELU, \\
  & CELU, SELU, GELU, GLU \\
  Dense Normalization &  None, BatchNorm, LayerNorm \\
  Dense Width &  32, 64, 128, 256, 512, 1024 \\
  Discount Factor &  0.1, 0.5, 0.9, 0.99, 0.995, 0.999\\
  Exploration $\epsilon$ &  0, 0.001, 0.005, 0.01, 0.1\\
  Learning Rate & 10, 5, 2, 1, 0.1, 0.01, 0.001, 0.0001, 1e-05 \\
  Minimum Replay History & 125, 250, 375, 500, 625, 750, 875, 1000 \\
  Number of Atoms & 11, 21, 31, 41, 51, 61\\
  Number of Convolutional Layers &  1, 2, 3, 4\\
  Number of Dense Layers &  1, 2, 3, 4 \\
  Replay Capacity &  \\
  Reward Clipping &  True, False\\
  Target Update Period &  10, 25, 50, 100, 200, 400, 800, 1600\\
  Update Horizon & 1, 2, 3, 4, 5, 8, 10\\
  Update Period &  1, 2, 3, 4, 8, 10, 12\\
  Weight Decay &  0, 0.01, 0.03, 0.1, 0.5, 1\\

  \midrule
  \end{tabular}
\end{table*}

\clearpage

\section{Interesting Miscellaneous Findings}
\label{sec:imf}
There were a couple of interesting findings from our experiments which are out of scope for this paper, but which may warrant further exploration in the future.

\subsection{High Values of Weight Decay Can Be Optimal}
We found that for DER at 40 Million environment frames having a weight decay of 0.1 was the overall best choice, and that for many games like Gopher and Boxing the optimal value was 0.5, an uncommonly high value for the hyperparameter. 
\begin{figure}[!ht]
    \centering
  \includegraphics[width=0.6\linewidth]{figures/DER_weight_decay_curves.pdf}%
    \caption{
     \textbf{Measured IQM of human-normalized scores on the 26 100k benchmark games, with varying Weight Decay} for DER. We evaluate performance at 100k agent steps (or 400k environment frames), and at 40 million environment frames. At 40 million frames 0.1 is on average optimal, with 0.5 being at second place and the standard value of 0.0 being in fourth.
    }
    \label{fig:imf_wd}
\end{figure}

\subsection{Higher Values of Adam's $\epsilon$ can improve Performance}
In our experiments we found that both DrQ($\epsilon$) and DER can benefit from a 100 times higher value of Adam's $\epsilon$ than what is commonly used. This is somewhat perplexing, as using such a high value of epsilon leads Adam to behave closer to SGD than to its common behaviour in other settings.
\begin{figure}[!ht]
    \centering
  \includegraphics[width=0.8\linewidth]{figures/DER_adam_eps.pdf}%
    \caption{
     \textbf{Measured IQM of human-normalized scores on the 26 100k benchmark games, with varying Adam's $\epsilon$} for DER. We evaluate performance at 100k agent steps (or 400k environment frames), and at 40 million environment frames.
    }
    \label{fig:imf_adam_eps}
\end{figure}

\subsection{Example Analysis: DER on Gopher}
\label{sec:gopher}
Finally we found that in a specific experimental setting, DER with 40 million frames on Gopher, whose optimal hyperparameters are very different from what is commonly observed in other applications of Deep Learning, and in some cases quite different even from the optimal values when using DER with 40 million environment frames in other Atari games. Not only that, but also we observed that often the difference in performance between the counter-intuitive optimal hyper-parameter and the standard is significant, leading to multiple-fold improvement in returns.
For example in Gopher specifically we find that:
\begin{itemize}
    \item For DER the standard value of update horizon is 10, but in the case of Gopher using an update horizon of 1 leads to roughly a 28 times improvement in performance.
    \item In Gopher a Weight Decay of 0.5 lead to a 5-fold increase of returns when compared to the standard value of 0.
    \item While the standard value of the Discount Factor is 0.99, for Gopher we see a 4.5 times improvement in performance when using a lower value of 0.9
    \item The optimal batch size we found was 4, which is relatively small compared to the standard of 32, and goes against the common Deep Learning practice of increasing batch sizes to increase performance. Changing batch size to 4 leads to a 4.5-fold increase in returns
    \item Finally, we recall the previous sub-section on Adam's $\epsilon$ and see that Gopher also benefits from an uncommonly high value of the hyperparameter, though here the performance gap is smaller, being closer to a 2x increase compared to the considerable differences discussed previously.
\end{itemize}

\begin{figure}[!ht]
    \centering
  \includegraphics[width=1.0\linewidth]{figures/gopher_weird.pdf}%
    \caption{
     \textbf{Learning Curves of DER on Gopher at 40M frames as we vary Adam's $\epsilon$, Weight Decay, Discount Factor, Update Horizon, and Batch Size}
    }
    \label{fig:weird_gopher}
\end{figure}