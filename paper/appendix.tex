
\section{Prompts for \ourmethod}
\label{app:prompts}
In this section, we list all prompts that we use for each step of our method.

\subsection{Fact Updating}
\label{app:update_prompts}
In practice, we implement these operations by performing \textit{two passes} over the retrieved facts.\bzl{TODO potentially abstract away this detail} In the first pass, we prompt the LM with the input $\doc$ and each fact $\fact\in R$ and prompt it to \textit{classify} the fact into one of \textit{reinforce, no change, make false}.
From this first pass, we divide the retrieved facts into two sets: $R_\text{true}$, comprising facts that remain true (\textit{reinforce, no change}), and $R_\text{false}$, comprised of facts that have become false (\textit{make false}).
In the second pass, we iterate through $R_\text{false}$, and prompt the LM to rewrite the fact into a true fact (if possible), conditioned on the new document $\doc$ and $R_\text{true}$. This serves a few purposes:
\begin{enumerate}
\item If $\fact$ is only made partially false by $\doc$, we may retain information expressed in $\fact$ but not $\doc$. For example, if $f$ is \textit{Mary and Bob work at UPS}, and $\doc$ is \textit{Mary got fired from UPS}, we may rewrite $\fact$ as \textit{Bob works at UPS}, rather than negating the entire fact.
\item Conditioning on $R_\text{true}$ allows the LM to make \textit{multi-hop} edits. For example, if $\fact$ is \textit{Mary is coworkers with Bob}, and $\doc$ is \textit{Mary changed workplaces to Amazon}, if $R_\text{true}$ contains \textit{Quinn works at Amazon}, then we can rewrite $\fact$ as \textit{Mary is coworkers with Quinn}.
\end{enumerate}


First round: classifying facts as becoming more or less likely to be true. 
\begin{lstlisting}
[Input] [Timestamp: {ts}] {context} [End Input]

The fact "{fact}" was previously true. In light of the input, is "{fact}" likely still true as of {ts}? Begin by summarizing the changes we learned from the input, then reasoning briefly about them to give your final answer with "Answer: Reinforce" (if the input makes the fact more likely) or "Answer: Make False" (if the input makes the fact less likely) or "Answer: No Change" (if the input doesn't affect the fact, e.g. if the input is irrelevant to the fact). Assume that the fact is still true (keep true) if nothing in the input contradicts it.
\end{lstlisting}

Second round: extracting rewrites
\begin{lstlisting}
[Input] [Timestamp: {ts}] {context}
Other True Facts at {ts}: {", ".join(still_true_facts)}
[End Input]

The fact "{fact}" was previously true but no longer. Given the above input and true facts, can you rewrite it into one that is true as of {ts}? Output your answer in form "rewrite: rewritten fact" or "no rewrite possible".
\end{lstlisting}

\subsection{Fact Extraction}
\label{app:fact_extraction}
\begin{lstlisting}
Extract all facts from the input text, with each fact on a new line and without bullet points or numbered lists. Facts should be simple, independent, standalone, and decontextualized. Break up long facts into smaller facts. Resolve all references (e.g. pronouns, definite articles, etc.) by copying full reference object everywhere it is referenced. Only include facts referring to the current world state (what is true *now*), as opposed to facts true in the past. If there are no facts, please output "No new facts." Do not include any other text.
\end{lstlisting}

\subsection{Inference}
\label{app:infer_prompt}
Given a question \texttt{question} at timestep \texttt{ts} (and choices \texttt{answer\_choices}), 
We first retrieve facts $\fact_i, [(\tau_{i0}, v_{i0}), (\tau_{i1}, v_{i1}), \cdots]$ from the knowledge base with similarity threshold $> 0.7$ to \texttt{question}. We then prompt a LM with the following: 
\begin{lstlisting}
Read the statements/passages below then answer the question below

***BEGIN STATEMENTS***
{f_i} ({v_{i0}} at {tau_{i0}}, {v_{i1}} at {tau_{i1}}, ...)
{f_j} ({v_{j0}} at {tau_{j0}}, {v_{j1}} at {tau_{j1}}, ...)
...
***END STATEMENTS***

Given the above statements are true and any prior knowledge you have, answer the following question at timestep {ts}?:
{question}

Briefly reason then answer with one of: {answer_choices}.
\end{lstlisting}

For questions requiring list answers (e.g. list all the siblings of Rachel), we replace the last line with:
\begin{lstlisting}
Briefly reason then answer with a JSON list, ["item1", "item2", ...], of zero or more of the following items: {answer_choices}. If you include any of the above items, make sure to copy their names exactly as is from the list. Your list may be empty, [], if none of the answers are true.
\end{lstlisting}




\section{Dataset Construction Details}
\subsection{News Articles}
\label{app:wikidata}

We construct this dataset in three stages:
\paragraph{Extracting World States $W$.} We retrieve \texttt{(subj,rel)} pairs from Wikidata for which there are at least two distinct fact relations at different timestamps, e.g. \verb|(subj,rel,obj1,start_ts1,end_ts1)| and \verb|(subj,rel,obj2,start_ts2,end_ts2)|. These timestamped facts are used to ``represent'' $W$. We filter for subjects \texttt{subj} located in English-speaking countries to ensure we can find English-language sources.
We use SPARQL\footnote{\url{https://www.w3.org/TR/sparql11-query/}} to obtain a set of \verb|(subj,rel)| pairs. 
    
\paragraph{Obtaining Documents $L$.} We annotate each timestamped relation, \verb|(subj,rel,obj,start_ts, end_ts)| with a source written between \verb|start_ts| and \verb|end_ts| (preferably close to the \verb|start_ts|) stating that the \verb|(subj,rel,obj)| relation is true. We crowdsource annotations from Prolific in two stages. In the first stage, Prolific annotators were presented with an interface which scraped candidate news articles off of Google\footnote{In particular, we set the to-be-matched parameter of the search to ``news'', i.e. \url{https://www.google.com/?tbm=nws}}, and were asked to select sources which stated that the fact \verb|(subj,rel,obj,start_ts, end_ts)| is true, but \textbf{did not} state that any succeeding fact, \verb|(subj,rel,obj2,start_ts2, end_ts2)| where \verb|start_ts2| $>$ \verb|start_ts|, is true.
In the second stage, we validated Prolific annotations from the first stage by presenting articles from the first round of annotations to annotators in the second round, and asking users whether those articles contained the fact in question. If second annotator does not affirm the fact is present in the article, we throw out the fact and the associated annotation.
\bzl{TODO: inter-annotator agreement}
We do an additional third round of filtration with a language model, asking the language model to affirm that the text of an article contains \verb|(subj,rel,obj,start_ts, end_ts)| but not any succeeding facts \verb|(subj,rel,obj2,start_ts2, end_ts2)|.
We only include articles and facts that pass all three rounds of annotation.
We recruited English-speaking participants from the US for annotations for all annotations. The full set of instructions we give annotators can be found in~\Cref{tab:annotator_instructions_1,tab:annotator_instructions_2}.
Screenshots of the interface can be found in~\Cref{fig:annotator_screenshot_1,fig:annotator_screenshot_2}.

\paragraph{Generating Question-Answers Pairs $(q,\{a\})$.} We automate generation of questions and answers from $W$ by writing templates for each relation and generating questions and answers from those templates. The full list of templates can be found in~\Cref{tab:wiki_qs_templates}.

\begin{table*}[]
    \centering
    \small
    \begin{tabular}{p{15cm}}
     \textbf{Please read these instructions carefully and only proceed once you have understood them. Once you start the task, you will have 10 minutes to get through as many questions as possible.}

    For each question, you will be presented a fact. Please find a news article that implies that the fact is true, according to the below requirements:
    \begin{enumerate}
    \item The article implies the fact, such that a reasonable person, without any prior knowledge, can infer that the fact is true from reading the article.

    Example: For fact Emad Mostaque is CEO of Stability AI (was True from 2020 to 2024-03-23)
        
    Good Sources: This startup is setting a DALL-E 2-like AI free, consequences be damned: Article says "...Stability AI CEO and founder Emad Mostaque wrote in a blog post"
    
    Bad Sources:  Artists can now opt out of the next version of Stable Diffusion: Cannot conclude fact from text of article

    \item The article is a news article or blog post.

    Example: For fact Taylor Aylmer is a member of the Racing Louisville FC sports tea

    Good Sources: Team News: Aylmer to make first regular season start

    Bad Sources: Taylor Aylmer - Racing Louisville FC Midfielder - ESPN, Taylor Aylmer - Instagram
         	
        
    \item The fact is stated in the main body of the article text, not in a table, list, image, image caption, embedded tweet, etc.

    Example: For fact Taylor Aylmer is a member of the Racing Louisville FC sports team

    Good Sources: Team News: Aylmer to make first regular season start, Recap: Racing rallies to beat Orlando, keep playoff hopes alive: Fact is in a list at the end, not the main text

    Bad Sources: Jaelin Howell, Racing Louisville bring community together to help people with Down syndrome: Fact is in an image caption but nowhere in the main text

    \item The article is a web page, not a PDF or other file format.

    Example: For fact Ali Shojaie is a IMS Fellow

    Good Sources: Ali Shojaie elected fellow of the Institute of Mathematical Statistics 

    Bad Sources: IMS Carver Award 2023: Source is a PDF file, not a web page
    
    \item The article is written in English.

    Example: For fact Emad Mostaque is CEO of Stability AI (was True from 2020 to 2024-03-23)

    Good Sources: This startup is setting a DALL-E 2-like AI free, consequences be damned

    Bad Sources: [Bengali article]: Article is not in English
    
    \item Avoid articles that state that the fact is or is about to become false. These are generally written near or past the end date of a fact being true.

    Example: For fact Emad Mostaque is CEO of Stability AI (was True from 2020 to 2024-03-23)

    Good Sources: This startup is setting a DALL-E 2-like AI free, consequences be damned

    Bad Sources: Stability AI founder Emad Mostaque plans to resign as CEO, sources say: Article is about the fact being about to be false
    \end{enumerate}

If no listed articles satisfy these requirements, you have the option to either find a news article that satisfies the requirements (a google search link is provided for reference, you may need to manually adjust the query or date parameters) or selecting "cannot find source" if you cannot find any source in a reasonable amount of time.

There may also be a second fact that you need to avoid. If you see this fact in the article, do not select it as a source.

\textbf{Tip}: You may use "ctrl-f" (find tool) to quickly validate whether or not a fact is in the article.
    \end{tabular}
    \caption{Instructions for round 1 of annotation for news article.}
    \label{tab:annotator_instructions_1}
\end{table*}
\bzl{non-English not showing up!}

\begin{table*}[]
\small
    \centering
    \begin{tabular}{p{15cm}}
\textbf{Please read these instructions carefully and only proceed once you have understood them. Once you start the task, you will have 12 minutes to get through as many questions as possible.}

For each question, you will be presented a fact and a news article. Please confirm that the news article implies that the fact is true, and conforms to the below requirements:
\begin{enumerate}
    \item The article implies the fact, such that a reasonable person, without any prior knowledge, can infer that the fact is true from reading the article.

    Example: For fact Emad Mostaque is CEO of Stability AI (was True from 2020 to 2024-03-23)
        
    Good Sources: This startup is setting a DALL-E 2-like AI free, consequences be damned: Article says "...Stability AI CEO and founder Emad Mostaque wrote in a blog post"
    
    Bad Sources:  Artists can now opt out of the next version of Stable Diffusion: Cannot conclude fact from text of article

    \item The article is written in English.

    Example: For fact Emad Mostaque is CEO of Stability AI (was True from 2020 to 2024-03-23)

    Good Sources: This startup is setting a DALL-E 2-like AI free, consequences be damned

    Bad Sources: [Bengali article]: Article is not in English

    \item Avoid articles that state that the fact is or is about to become false. These are generally written near or past the end date of a fact being true.

    Example: For fact Emad Mostaque is CEO of Stability AI (was True from 2020 to 2024-03-23)

    Good Sources: This startup is setting a DALL-E 2-like AI free, consequences be damned

    Bad Sources: Stability AI founder Emad Mostaque plans to resign as CEO, sources say: Article is about the fact being about to be false
\end{enumerate}

If the provided article does not satisfy these requirements, you have the option to either find a news article that satisfies the requirements (a google search link is provided for reference, you may need to manually adjust the query or date parameters) or selecting "cannot find source" if you cannot find any source in a reasonable amount of time.

There may also be a second fact that you need to avoid. If you see this fact in the article, do not select it as a source.

\textbf{Tip}: You may use "ctrl-f" (find tool) to quickly validate whether or not a fact is in the article.

    \end{tabular}
    \caption{Instructions for round 2 of annotation for news article.}
    \label{tab:annotator_instructions_2}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/round1_interface_screenshot.png}
    \caption{Screenshot of round 1 of annotation for news article.}
    \label{fig:annotator_screenshot_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/round2_interface_screenshot.png}
    \caption{Screenshot of round 2 of annotation for news article.}
    \label{fig:annotator_screenshot_2}
\end{figure*}

\begin{table*}[]
\small
    \centering
    \begin{tabular}{p{6cm}p{12cm}}
\multirow{2}{6cm}{
    \texttt{(\{subj\}, employer, \{obj\})}
} & \verb|Who is the employer of {subject}?| \\
& \verb|Is {subject} an employee of {object}?| \\
\multirow{3}{6cm}{
    \texttt{(\{subj\}, chief executive officer, \{obj\})}
} & \verb|Who is the CEO of {subject}?| \\
& \verb|What company is {object} the CEO of?| \\
& \verb|Is {object} the CEO of {subject}?| \\
\multirow{3}{6cm}{
    \texttt{(\{subj\}, chairperson, \{obj\})}
} & \verb|Who is the chairperson of {subject}?| \\
& \verb|What organization is {object} the chairperson of?| \\
& \verb|Is {object} the chairperson of {subject}?| \\
\multirow{3}{6cm}{
    \texttt{(\{subj\}, head of state, \{obj\})}
} & \verb|Who is the head of state of {subject}?| \\
& \verb|Where is {object} the head of state of?| \\
& \verb|Is {object} the head of state of {subject}?| \\
\multirow{2}{6cm}{
    \texttt{(\{subj\}, position held, \{obj\})}
} & \verb|What government position does {subject} hold?| \\
& \verb|Does {subject} hold government position {object}?| \\
\multirow{2}{6cm}{
    \texttt{(\{subj\}, member of sports team, \{obj\})}
} & \verb|What sports team is {subject} a member of?| \\
& \verb|Is {subject} a member of {object}?| \\
\multirow{3}{6cm}{
    \texttt{(\{subj\}, unmarried partner, \{obj\})}
} & \verb|Who is the unmarried partner of {subject}?| \\
& \verb|Who is the unmarried partner of {object}?| \\
& \verb|Is {object} the unmarried partner of {subject}?| \\
\multirow{2}{6cm}{
    \texttt{(\{subj\}, residence, \{obj\})}
} & \verb|Where does {subject} reside?| \\
& \verb|Does {subject} reside in {object}?| \\
\multirow{2}{6cm}{
    \texttt{(\{subj\}, headquarters location, \{obj\})}
} & \verb|Where is the headquarters location of {subject}?| \\
& \verb|Is the headquarters location of {subject} in {object}?| \\
\multirow{2}{6cm}{
    \texttt{(\{subj\}, P463, \{obj\})}
} & \verb|What organization is {subject} a member of?| \\
& \verb|Is {subject} a member of {object}?| \\
\multirow{2}{6cm}{
    \texttt{(\{subj\}, member of political party, \{obj\})}
} & \verb|What political party is {subject} a member of?| \\
& \verb|Is {subject} a member of {object}?| \\
    \end{tabular}
    \caption{Question-answer templates in the News domain}
    \label{tab:wiki_qs_templates}
\end{table*}


\paragraph{Prolific Details}


We recruited a total of 680 English-speaking prolific annotators from the United States, with each annotator spending an average of 16:50 minutes on the task ($\sim$ 7 minutes to read and understand instructions). We paid annotators an average of $\$14.20$ per hour.
This task was deemed exempt from IRB review.
No personally-identifiable information was collected or stored, and all prolific annotators were associated with an anonymous prolific ID.


\subsection{Synthetic Conversations}
\label{app:convos}
We also construct this dataset in three stages:
\paragraph{Generating World States $W$.} We model the underlying world and its transformations as a Markov chain with states $S$ and a set of allowable transitions $T(S)$ determined by $S$. At each timestep, we randomly sample a transition from $T(S)$ uniformly at random.
States $S$ are described by a set of relations \verb|(subj, rel, obj)|. 
The full list of entities types and relations for each entity type can be found in~\Cref{tab:convo_states}. To construct each world, we subsample 10 people and 5 companies, and randomly initialize their kinship and employment relations.
Transitions $t\in T(S)$ change one or more relation in the state. To be able to test the limits of our propagation, the set of transitions we define in this domain all change more than one relation: for example, ``\textit{Bob changed jobs to work at Google}'' changes the \textit{employees} of Google, the set of \textit{coworkers} of Bob, the set of \textit{coworkers} of all Google employees, and the set of \textit{coworkers} of all employees of Bob's former company, etc. The full list of transitions and their downstream effects can be found in~\Cref{tab:convo_transitions}.

\paragraph{Generating Conversations $L$.} We generate conversations by sampling two people in the world $p_1$ and $p_2$ and prompting two LLMs with their corresponding personas and initial facts.
We then generate twelve conversation ``chunks'' as follows:
We begin by sampling the next transition we want to make in the world.
The transition corresponds to a natural language string that corresponds to only a single relation. However, we know that each transition is associated with multiple changing relations. To be able to infer the \textit{downstream} changes of a single relation changing, we need to know auxiliary facts related to the \textit{object} of the changed relation.
In the multi-hop subset of this dataset, we mention auxiliary facts in the \textit{prior} conversation chunks, while only mentioning the immediate transition (on a single relation) in the current chunk (\textit{without} mentioning any downstream changes). Thus, to make the correct downstream inferences on this subset, the system must retrieve and reason across facts from prior conversation chunks.

For the singlehop subet, we mention \textit{all downstream effects} in the same conversation chunk that a transition is made.

\begin{table*}[]
    \centering
    \begin{tabular}{p{2cm}p{13cm}}
    \toprule
        Entity Type & Possible Relations \\
        \midrule
        Person & spouse, parents, children, job, company, hobbies, coworkers, work location, boss, salary, industry, is-employed-full-time, work hours, workplace, siblings, parents-in-law, children-in-law, step-parents, step-children, equipment necessary for hobbies \\
        Company & employees, jobs, head, location, industry, workplace type \\
        Job & company, salary, is-full-time, work hours \\
        Hobby & equipment necessary for hobby \\
        \bottomrule
    \end{tabular}
    \caption{Full list of entities and relations defining each world state in the Conversation domain.}
    \label{tab:convo_states}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{p{4cm}p{10cm}}
    \toprule
        Transition type & Downstream effects \\ \midrule
        \texttt{person.job} changes from \texttt{job1} to \texttt{job2} & 
        person.company, person.coworkers, person.work-location, person.boss, person.salary, person.industry, person.is-employed-full-time, person.work-hours, person.workplace, job1.company.employees, job2.company.employees \\ \midrule
        \texttt{person.spouse} changes from \texttt{person1} to \texttt{person2} & person.parents-in-law, person.parents.children-in-law, person.children.step-parents, person.step-children, person1.spouse, person1.parents-in-law, person1.parents.children-in-law, person2.spouse, person2.parents-in-law, person2.parents.children-in-law, person2.children.step-parents, person2.step-children \\ \midrule
        \texttt{person} adopts \texttt{child} & person.children, child.parents, child.siblings, child.spouse.parents-in-law, person.children-in-law, child.step-parents, person.spouse.step-children, person.children.siblings \\ \midrule
        \texttt{person} gets a new hobby \texttt{hobby} & person.equipment-necessary-for-hobbies \\ \midrule
        \texttt{job.salary} changes & for all people that have that job: person.salary \\ \midrule
        \texttt{job.work-hours} changes & for all people that have that job: person.work-hours \\
        \bottomrule
    \end{tabular}
    \caption{Full list of possible state transitions in the Conversation domain. Note the set of available transitions may vary depending on the underlying state.}
    \label{tab:convo_transitions}
\end{table*}



\paragraph{Generating Question-Answers Pairs $(q,\{a\})$.} Given a world state at time $t$, we query \textit{all} facts about the world. Similar to the news setting, we automate generation of questions and answers through templates. Templates in this setting can be found in~\Cref{tab:convo_qs_templates}.


\begin{table*}[]
\small
    \centering
    \begin{tabular}{p{8cm}p{7cm}}
\multirow{2}{8cm}{
    \texttt{(\{subj\}, spouse, \{obj\})}
} & \verb|Who is the spouse of {subj}?| \\
& \verb|Who is the spouse of {obj}?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, job, \{obj\})}
} & \verb|What is the job of {subj}?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, company, \{obj\})}
} & \verb|Which company does {subj} work at?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, hobbies, \{obj\})}
} & \verb|List all known hobbies of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, coworkers, \{obj\})}
} & \verb|List all known coworkers of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, work location, \{obj\})}
} & \verb|In which city does {subj} work?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, boss, \{obj\})}
} & \verb|Who is the head of {subj}'s workplace?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, salary, \{obj\})}
} & \verb|What is the salary of {subj}?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, industry, \{obj\})}
} & \verb|What industry does {subj} work in?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, is-employed-full-time, \{obj\})}
} & \verb|Does {subj} work full-time or part-time?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, work-hours, \{obj\})}
} & \verb|What are the work hours of {subj}?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, workplace, \{obj\})}
} & \verb|What type of workplace does {subj} work out of?| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, parents, \{obj\})}
} & \verb|List all parents of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, children, \{obj\})}
} & \verb|List all children of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, siblings, \{obj\})}
} & \verb|List all siblings of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, parents-in-law, \{obj\})}
} & \verb|List all parents-in-law of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, children-in-law, \{obj\})}
} & \verb|List all children-in-law of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, step-parents, \{obj\})}
} & \verb|List all step-parents of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, step-children, \{obj\})}
} & \verb|List all step-children of {subj}.| \\
\multirow{1}{8cm}{
    \texttt{(\{subj\}, necessary equipment for hobby, \{obj\})}
} & \verb|List all equipment {subj} needs for their hobbies.| \\
    \end{tabular}
    \caption{Question-answer templates in the Conversation domain}
    \label{tab:convo_qs_templates}
\end{table*}


\begin{table}[!t]
    \centering
    \footnotesize
    \begin{tabular}{lcc}
    \toprule
       Relation type  & \# \texttt{(s, r)} & \# \texttt{(s, r, o)} \\
       \midrule
       Member of sports team & 284 & 382 \\
       Position held & 164 & 382 \\
       Employer & 38 & 77 \\
       Chairperson & 20 & 42 \\
       Head of state & 9 & 18 \\
       CEO & 6 & 13 \\
       Unmarried partner & 5 & 12 \\
       Residence & 4 & 8 \\
       Headquarters & 2 & 4 \\
       Member of political party & 1 & 2 \\
       \midrule
       Total & 533 & 1174  \\
       \bottomrule
    \end{tabular}
    \caption{Breakdown of changed relation types in the News domain, categorized by number of unique \texttt{(subj, rel)} pairs and \texttt{(subj, rel, obj)} triples.}
    \label{tab:news_relations}
\end{table}


\section{Dataset Statistics}
\label{app:dataset_stats}
The breakdown of changes in each of our datasets can be found in~\Cref{tab:news_relations} for news articles and~\Cref{fig:convos_stats} for conversations. The breakdown of questions for conversations can be found in~\Cref{tab:convos_questions}.


\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
    \footnotesize
    \begin{tabular}{lccc}
    \toprule
       Question Topic & \# Yes/No & \# Multiple Choice &  \# MC Choices \\
       \midrule
       Boss & 140 & 74 & 26\\
       Coworkers & 481 & - & -\\
       Industry & - & 74 & 26 \\
       Is employed full-time & 82 & - & -\\
       Salary & 158 & 80 & 11 \\
       Work hours & 110 & 64 & 10 \\
       Work Location & 274 & 72 & 20 \\
       Workplace & 140 & 74 & 26 \\
       \midrule
       Total & 1385 & 438  \\
       \bottomrule
    \end{tabular}
    }
    \caption{Distribution of generated questions in the Synthetic Conversation domain, categorized by question topic and type.}
    \label{tab:convos_questions}
\end{table}


\begin{figure*}
        \centering
        \includegraphics[width=\linewidth]{figures/transitions_overview.pdf}
        \caption{Distribution of changed relation types in the (A) News and (B) Conversation domains. (A) depicts the number of transitions per year of each month, while (B) depicts the number of relation types that underwent that number of changes across all conversations.}
        \label{fig:convos_stats}
\end{figure*}

\section{Qualitative Analysis}


\subsection{Error Analysis: Conversations (Multihop)}
\label{sec:multihop_errors}
Prototypical examples of multihop edit errors can be found below:

\paragraph{Retrieval Errors} Sometimes, the full list of facts that need to be updated are not retrieved. For example:

Input conversation chunk:
\begin{quote}
\it{
2023-11-01

Katie: Hey Olivia! How have you been? Guess what? I've changed my job to Library Assistant at Central Public Library! What's new with you?

Olivia: Hey Katie! That's amazing news! Congrats on the new job as a Library Assistant at Central Public Library!

...
}
\end{quote}

Retrieved facts:

\begin{itemize}
\it{
\item The role of General Practitioner at HealthFirst Medical Clinic is a full-time job.
\item The role of Library Assistant at Central Public Library is a full-time job.
\item Rachel has a full-time job as a Medical Assistant at HealthFirst Medical Clinic.
\item Rachel works at HealthFirst Medical Clinic as a Medical Assistant.
\item The work hours of a Library Assistant at Central Public Library are from 9 to 17.
\item Katie works full-time at the Urban Development Project.
\item Olivia works full-time at the airport.
\item The salary for a Library Assistant at Central Public Library is \$80,000.
\item Peter works from 9 to 17 at the Central Public Library.
\item Diana and Liam both have full-time jobs at the Urban Development Project.
\item The salary of an Archivist at Central Public Library is \$130,000.
}
\end{itemize}

The retriever at step 1 failed to retrieve the following facts to update. At inference time, these facts were retrieved and affected the LM's answer on questions:
\begin{itemize}
{\color{red}
\it{
\item Katie's working hours are from 10 to 15.
\item Katie's salary is \$100000.
\item Diana and Liam work with Katie.
}
}
\end{itemize}



\paragraph{Updating Errors}
Sometimes, facts that needed to be updated were retrieved, but were not updated by the LM. For example,

Input conversation chunk:
\begin{quote}
\it{
2023-11-26

Katie: Hey Olivia! How have you been? I heard you changed your job to Safety Officer at Urban Development Project! How's the new role treating you?

Olivia: Hey Katie! Yes, I did change my job to Safety Officer at Urban Development Project! The new role has been quite a change, but I'm really enjoying it so far. How have you been?

...
}
\end{quote}

Retrieved fact:

\begin{itemize}
\it{
\item Olivia works from 9 to 17 at her job.
}
\end{itemize}

Model-outputted update (and reasoning):
\begin{quote}
\begin{lstlisting}
Summary of changes:


    Olivia changed her job to Safety Officer at Urban Development Project.

    No information is provided about Olivia's work schedule.


Reasoning:

The input does not provide any information that would suggest Olivia's work schedule has changed. The conversation is about Olivia's new job, Katie's hobbies, and their plans for social activities. There is no mention of Olivia's work schedule or any changes that would affect it. Therefore, the fact "Olivia works from 9 to 17 at her job" is likely still true as of 2023-11-01.

Answer: No Change
\end{lstlisting}
\end{quote}

Note that this fact should've been flagged for potential updating and forwarded to the rewrite step (where the LM has an opportunity to retrieve more facts to confirm or deny this fact). 
However, the LM does not classify this fact as \textit{make false} because it is not directly contradicted by the input.



\section{Use of AI Assistants}
Code was written with Co-pilot turned on. GPT* models were also consulted for creating acronyms for the method and dataset names.