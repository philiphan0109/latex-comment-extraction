Section: Other
  Comment Indices: (75, 101)
  Comment Text: 
 % \usepackage{algorithmic}

  Comment Indices: (103, 127)
  Comment Text: 
 % \usepackage{algorithm}

  Comment Indices: (147, 224)
  Comment Text: 
 % \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}

  Comment Indices: (513, 555)
  Comment Text: 
 % updated with editorial comments 8/9/2021

  Comment Indices: (584, 592)
  Comment Text: 
 % colors

  Comment Indices: (770, 817)
  Comment Text: 
 % For align environment and other math features

  Comment Indices: (969, 1098)
  Comment Text: 
 %\input{latex_resources/my_sections.tex}
%\input{latex_resources/my_beamer_defs.sty}
%\input{latex_resources/plot_appearance.tex}

  Comment Indices: (1358, 1390)
  Comment Text: 
 % <-this % stops a space

  Comment Indices: (1710, 1734)
  Comment Text: 
 % <-this % stops a space

  Comment Indices: (1735, 1806)
  Comment Text: 
 % \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}

  Comment Indices: (1810, 2000)
  Comment Text: 
 % The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

  Comment Indices: (2002, 2181)
  Comment Text: 
 % \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

Section: Abstract
Section: Introduction
  Comment Indices: (6900, 6956)
  Comment Text: 
 % \begin{wrapfigure}{r}{0.5\textwidth}
% 	\begin{center}

  Comment Indices: (7041, 7056)
  Comment Text: 
 % \end{center}

  Comment Indices: (7258, 7276)
  Comment Text: 
 % \end{wrapfigure}

  Comment Indices: (8866, 9259)
  Comment Text: 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  Comment Indices: (9291, 9326)
  Comment Text: 
 %\begin{subfigure}{0.38\textwidth}

  Comment Indices: (9392, 9606)
  Comment Text: 
 %\subcaption{}
	%\end{subfigure}
	%\quad
	%\begin{subfigure}{0.58\textwidth}
	%\includegraphics[width = \textwidth]{figures/one_shot_aggregation} 
	%\subcaption{}    
	%\label{fig:dom_parallel_b}
	%\end{subfigure}

  Comment Indices: (12090, 12222)
  Comment Text: 
 %(ii) the size of the partition is as big as a single unit can process it, which forces servers to be above a particular threshold.

Section: Learning Graph Neural Networks
  Comment Indices: (15257, 15398)
  Comment Text: 
 %given by $[\bbS]_{ij}=w(i,j)/m$, where $w_{ij}$ is the weight corresponding to edge $(i,j)$, with $w\in\{0,1\}$ if the graph is unweighted.

  Comment Indices: (18006, 18007)
  Comment Text: 
 %

  Comment Indices: (18126, 18128)
  Comment Text: 
 %

  Comment Indices: (18973, 18974)
  Comment Text: 
 %

  Comment Indices: (19619, 19620)
  Comment Text: 
 %

  Comment Indices: (19769, 19770)
  Comment Text: 
 %

  Comment Indices: (20323, 20973)
  Comment Text: 
 % Let ${\bf h}^{l-1}$ be the tensor of vertex features at layer $l-1$ and ${\bf h}^0 \equiv {\bf x}$ the input vertex features. The action of a GNN layer is given by:
% \begin{align}
	%   {\bf z}^l & = {\bf W}^l{\bf h}^{l-1},\\
	%   m^l_{j\rightarrow i} & = f_{message}(z_i^l,z_j^l;\ccalH_l), \\
	%   h_i^l & = f_{node}(h_i^{l-1},Agg(\{m^l_{j\rightarrow i} : j\in \mathcal{N}(i)\});\phi^l), \\
	% \end{align}
% where ${\bf W}^l$ is a learnable weight matrix. $f_{message}$ and $f_{node}$ are learnable transformations parameterized by $\ccalH_l$ and $\phi_l$, respectively. $Agg$ is a a permutation-invariant aggregation operator such as mean or sum.

Section: Distributed GNN training
  Comment Indices: (21010, 22171)
  Comment Text: 
 % We consider two different distributed training scenarios: distributed full-graph training and distributed sampling-based training. In distributed full-graph training, we partition the graph as shown in Fig.~\ref{fig:dom_parallel_a}, where both the graph topology (the edges) and the input node features are partitioned almost equally among the training machines. Each machine is responsible for computing the features of the vertices in its local partitions for all layers in the GNN layer stack. The GNN model is replicated across all machines. The communication steps for a single GNN layer are illustrated in Fig.~\ref{fig:dom_parallel_b}: in the forward pass, each machine fetches the feature vectors of remote neighbors and propagates their messages to the nodes in the local partition; in the backward pass, gradients flow in the opposite direction and are accumulated in the GNN model weights. At the end of the backward pass, the weight gradients are summed across all machines and used to update the GNN model weights. In every GNN layer, our compression schemes are used to compress remotely fetched features. The message function thus has the form:

  Comment Indices: (26556, 26687)
  Comment Text: 
 %\State {\textbf{Each Worker $w_i$}: Decompress activations $\tilde %x_i=g^{-1}_{\epsilon,r_k}(a_i)$, and compute gradient step.}

  Comment Indices: (28742, 28744)
  Comment Text: 
 %

  Comment Indices: (29519, 29520)
  Comment Text: 
 %

  Comment Indices: (29732, 30458)
  Comment Text: 
 % \begin{assumption}\label{as:estimator_variace}
% 	The empirical estimator of the gradient $ \nabla_\ccalH \ell (y_i,\Phi(x_i,\bbS;\ccalH_t))$ is an unbiased estimator of the gradient $\nabla_\ccalH \ell (y_i,\Phi(x_i,\bbS;\ccalH_t))$, and the variance is  $\sigma^2$,
% 	\begin{align}
% 		\mbE\bigg[|| \nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t))- \nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t))||^2\bigg]\leq \sigma^2.
% 	\end{align}
% 	% \begin{align}
% 		% 	\mbE\bigg[|| \frac{1}{B}\sum_{i=1}^B\nabla_\ccalH \ell (y_i,\Phi(x_i,\bbS;\ccalH_t))- \nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t))||^2\bigg]\leq \frac{\sigma^2}{B}
% 		% \end{align}
% 	% with $\infty>\sigma>0$ being the variance of the estimator. 
% \end{assumption}

  Comment Indices: (31073, 31124)
  Comment Text: 
 % \red{add $\sigma$ variance, normalized input X}
%

  Comment Indices: (31225, 31357)
  Comment Text: 
 %Assumption \ref{as:estimator_variace} is true for i.i.d. data, the exact characterization of $\sigma$ depends on the architecture.

  Comment Indices: (32931, 33449)
  Comment Text: 
 % \begin{figure*}
% 	\begin{subfigure}{0.5\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/prodsrandom16.pdf}
% 		\caption{Random Partitioning}
% 		\label{fig:sub1}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.5\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/prodsmetis16.pdf}
% 		\caption{METIS Partitioning}
% 		\label{fig:sub2}
% 	\end{subfigure}
% 	\caption{Accuracy per epoch for the Products Dataset with $16$ servers.}
% 	\label{fig:prods16epoch}
% \end{figure*}

  Comment Indices: (33653, 33654)
  Comment Text: 
 %

  Comment Indices: (34267, 34321)
  Comment Text: 
 %, as well as the variance of the estimator $\sigma$.

Section: VARCO - Variable Compression For Distributed GNN Learning
  Comment Indices: (35633, 35636)
  Comment Text: 
 %
%

  Comment Indices: (36654, 37201)
  Comment Text: 
 % \begin{figure*}
% 	\begin{subfigure}{0.5\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/arxivrandom16.pdf}
% 		\caption{Random Partitioning}
% 		\label{subfig:arxivRandom}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.5\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/arxivmetis16.pdf}
% 		\caption{METIS Partitioning}
% 		\label{subfig:arxivmetis}
% 	\end{subfigure}
% 	\caption{Accuracy as a function of epoch for the Arxiv Dataset with $16$ servers.}
% 	\label{fig:arxiv16epoch}
% \end{figure*}

  Comment Indices: (37203, 38050)
  Comment Text: 
 % \begin{figure}[h]
%   \centering
%   \begin{subfigure}{0.33\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/all_partitionsarxivrandom.pdf}
%     \caption{Random Part. in Arxiv Dat.}
%     \label{subfig:ServerRandomArxiv}
%   \end{subfigure}
%   \begin{subfigure}{0.33\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/all_partitionsarxivmetis.pdf}
%     \caption{METIS Part. in Arxiv Dat.}
%     \label{subfig:ServerMetisArxiv}
%   \end{subfigure}
%   \begin{subfigure}{0.33\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/all_partitionsprodsrandom.pdf}
%     \caption{Random Part. in Products Dat.}
%     \label{subfig:ServerRandomProds}
%   \end{subfigure}
%   \caption{Accuracy as a function of the number of servers.}
%   \label{fig:servers}
% \end{figure}

  Comment Indices: (38052, 38637)
  Comment Text: 
 % \begin{figure}[h]
%   \centering
%   \begin{subfigure}{0.5\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/arxivrandom16.pdf}
%     \caption{Random Partitioning}
%     \label{subfig:arxivRandom}
%   \end{subfigure}%
%   \begin{subfigure}{0.5\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/arxivmetis16.pdf}
%     \caption{METIS Partitioning}
%     \label{subfig:arxivmetis}
%   \end{subfigure}
%   \caption{Accuracy as a function of epoch for the Arxiv Dataset with $16$ servers.}
%   \label{fig:arxiv16epoch}
% \end{figure}

  Comment Indices: (38639, 38686)
  Comment Text: 
 % \subsection{$\algo$ Algorithm Construction}
%

  Comment Indices: (39559, 39560)
  Comment Text: 
 %

Section: Experiments
  Comment Indices: (45237, 45505)
  Comment Text: 
 % Also, the fixed compression scheme is not able to recover the full communication accuracy when the number of servers increases. Consistent with Proposition \ref{prop:fixed_compression}, as the fixed compression increases, the accuracy attained by the GNN decreases.

  Comment Indices: (46155, 47658)
  Comment Text: 
 % We study the accuracy as a function of the number of epochs with $16$ servers. This setup is the most challenging, given that the size of the graph in each server is the smallest, and it is therefore the one in which communication is needed.  
% In Figure \ref{fig:arxiv16epoch}, we show the accuracy per epoch for the Arxiv dataset. As can be seen in both random \ref{subfig:arxivRandom}, and METIS \ref{subfig:arxivmetis} partitioning, the accuracy of variable compression is comparable to the one with full communication. Also, the different fixed compression mechanisms have a worse performance (10\% and 3\% in random and METIS partitioning respectively), and their performance degrades as their fixed compression increases. 
% In Figure~\ref{fig:prods16epoch}, we plot the results for the products dataset with $16$ servers. Again, in both partitioning schemes, our variable compression algorithm attains a comparable performance to the one trained on full communication. In this case, compared to the Arxiv dataset \ref{fig:arxiv16epoch}, the spread of the results is smaller, and the effect of our method is less significant. This is related to the fact that the graph is larger, and therefore, the partitions are also larger.
% In all, for both partitioning methods, and both datasets, we can validate that the variable compression mechanism attains a comparable performance to the one trained on the full communication, which is not the case for fixed compression. 
% \subsection{Efficiency}

  Comment Indices: (47870, 47871)
  Comment Text: 
 %

  Comment Indices: (49400, 53181)
  Comment Text: 
 %In Figure \ref{fig:accuracy_floats}, we show the accuracy as a function of the communicated floating points values for OGBN-Arxiv with $16$ machines. In Figure \ref{fig:accuracy_floats}, we show that $\algo$
%obtains the most efficient training curve in terms of communication costs, given that the variable compression line is above all other curves. 
%Intuitively, all learning curves show a similar slope at the beginning of training, and they converge to different values in the later stages, decreasing the compression rate is an efficient way of learning a GNN as shown in Figure \ref{fig:accuracy_floats}. Given that the $\algo$ curve in Figure \ref{fig:accuracy_floats} is above all curves, this means that for any communication budget i.e. number of bits, $\algo$ obtains the best accuracy of all methods considered. This indeed validates our claim that using $\algo$ is an efficient way of training a GNN. 
%In all, our experiments on real-world datasets show that $\algo$ is an efficient way of obtaining GNNs that attain a comparable performance to the one obtained using full communication, but, at a fraction of the communication costs. In Appendix \ref{appendix:scheduler}, we show different combinations of variable compressions, all of which attain same similar results, showcasing the robustness of our algorithm.
% \subsection{Robustness}
% In Figure \ref{fig:robustness} we validate the robustness of our method to the choice of compression rate. Using linear compression rate, at epoch $e$, with $c_{min}=1$, $c_{max}=128$, $E=300$ and compression rate $c =\min(c_{max} - a \frac{ c_{max} - c_{min}}{E}e, c_{min} )$, we vary the slope $a$ of the scheduler and verify that the algorithm attains a comparable performance for all runs.  This is consistent with Proposition \ref{prop:scheduler}, as we only require that the scheduler decreases in every iteration. In Appendix \ref{appendix:CompressionMechanism}, the equations that govern the compression rate are described. 
% \begin{figure*}
% 	\begin{subfigure}{0.33\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/robustnessarxivrandom16.pdf}
% 		\caption{Random Part. in Arxiv Dat.}
% 		\label{subfig:RobustRandomArxiv}
% 	\end{subfigure}
% 	\begin{subfigure}{0.33\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/robustnessprodsrandom16.pdf}
% 		\caption{Random Part. in Prods Dat.}
% 		\label{subfig:RobustMetisArxiv}
% 	\end{subfigure}
% 	\begin{subfigure}{0.33\textwidth}
% 		\centering
% 		\includegraphics[width=\linewidth]{figures/schedulers/comm_floats_per_epoch.pdf}
% 		\caption{Comm. Floats Per Epoch}
% 		\label{subfig:ServerRobustEpochs}
% 	\end{subfigure}
% 	\caption{Accuracy per epoch with $16$ servers for different variable compression schemes.}
% 	\label{fig:robustness}
% \end{figure*}
%In Table \ref{table:results} we show the results of our Algorithm $\algo$. For all configurations, $\algo$ attains a comparable performance to the full communication setup. It should also be noted that $\algo$ is robust given that it does not depend on the the number of servers or the dataset. Table \ref{table:results} therefore validates the claim that our $\algo$ attains a comparable performance to the one obtained with full communication. 
%\begin{wrapfigure}{r}{0.5\textwidth}
%	\begin{center}
%		\includegraphics[width=0.5\textwidth]{figures/accuracy/arxiv16floats.pdf} 
%	\end{center}
%	\caption{Accuracy of the GNNs as a function of the communicated floats for the OGBN-Arxiv with $16$ machines.}
%	\label{fig:accuracy_floats}
%\end{wrapfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Section: Conclusion
  Comment Indices: (54067, 54097)
  Comment Text: 
 % \bibliographystyle{unsrtnat}

