\appendix
\section{Compression Mechanism}
\label{appendix:CompressionMechanism}
% We use a single linear layer (with no non-linearity) to project $F$-dimensional features to $F_C$ dimensional features on the sender side, where $F_C < F$. On the receiver side, we use another linear layer to project back to the original $F$-dimensional feature space, i.e, $x_{decompress} = {\bf W}_{decoder}{\bf W}_{encoder} x$, where $x$ and $x_{decompress}$ are the original and reconstructed feature vectors, respectively. ${\bf W}_{decoder}$ and ${\bf W}_{encoder}$ are the $F \times F_C$, and $F_C \times F$ decoder and encoder projections, respectively. ${\bf W}_{decoder}$ and ${\bf W}_{encoder}$ are trained, together with the GNN parameters, to minimize task loss.

% In the variale compression scenario, we gradually decrease the compression ratio as training progresses. The transmittted compressed feature tensor's size increases by  $\Delta F$ every few training epochs. We achieve this by increasing the number of rows of ${\bf W}_{encoder}$ and the number of columns of ${\bf W}_{decoder}$ by $\Delta F$. The new rows in ${\bf W}_{encoder}$ are initialized using Glorot initialization. To avoid a sudden jump in the network response, we initialize the new columns of ${\bf W}_{decoder}$ to zeros.  

For the compression mechanism, we communicate the total number of elements in the feature vector and intermediate activations divided by the compression ratio. Which values of the vectors to communicate are chosen at random at the encoder's end. 
For the decoder to know which element of the vector corresponds to the true values, a random key generator is shared a priori. The decoder simply places the values communicated in the corresponding position and sets a $0$ on the rest of the non-communicated values.






% \subsection{Deterministic Sliding Window}
% \subsection{Random Sampling}


% \begin{figure*}[h]
	% 	\centering
	% 	\begin{subfigure}{\textwidth}
		% 		\includegraphics[width = \textwidth]{figures/Architecture.png} 
		% 		\subcaption{}
		% 		\label{fig:arch_fixed}
		% 	\end{subfigure}
	% 	% \begin{subfigure}{\textwidth}
		% 		% 	\includegraphics[width = \textwidth]{figures/variable_compression.png} 
		% 		% 	\subcaption{}
		% 		% 	\label{fig:arch_variable}
		% 		% \end{subfigure}
	% 	\caption{Model architecture for $f_{compress}$ with (a) fixed compression ratio, and, (b) variable compression ratio.}
	% \end{figure*}
% \subsection{Network Architecture for $f_{compress}$ with fixed $r$}
% We introduce a learnable feature-based compression-decompression routine $f_{compress} = g^{-1} \circ g$ which employs an autoencoder architecture, with the encoder approximating the compressor function $g$ and the decoder approximating the decompressor function $g^{-1}$. We adopt a multilayer perceptron design for both the encoder and decoder, which is shared across all nodes in the graph. In this routine, when the worker $W_i$ requests for the node features $x_{v^i_j} \in \reals^m$ to a remote worker $W_j$, $W_j$ first employs the encoder function $g$ to embed the node features into a latent space $z_{v^i_j} \in \reals^n$ and then sends them to $W_i$. Upon receiving, $W_i$ uses the decoder function $g^{-1}$ on $z_{v^i_j}$ to decompress the received features into ${x}_{v^i_j}$. To train $g$ and $g^{-1}$ in an end-to-end manner, we use the original downstream task loss (e.g. cross-entropy loss for classification task) instead of the typical reconstruction loss utilized in an autoencoder training. In fact, the reconstruction loss can't be computed because the original feature vectors $x_{v^i_j}$ are not shared with machine $W_i$.

% \subsection{Network Architecture for $f_{compress}$ with variable $r$}
% For variable compression ratio $r=r_t$, the encoder needs to generate output vectors of variable sizes $n=n_t$ where $t$ is the training time. To facilitate this into the autoencoder architecture, we add an extra dropout layer at the end of the encoder with a variable dropout probability $p_t = 1 - \frac{1}{r_t}$. On the decompressor side, we add an extra linear layer at the front of the decoder and scale the activations by $\frac{1}{1-p_t} = r_t$. This layer ensures that the expected input values to the decoder don't change with $r_t$ and thus the training is stable.




\subsection{Scheduler}
\label{appendix:scheduler}
Several strategies can be utilized to increase the compress rate as we learn. A simple strategy is to increase it a fixed rate $r_{k+1}=r_k+R$, where $R$ is the fixed rate. Another strategy is to implement linear increase $r_{k}=\alpha k+r_0$, where $\alpha>0$ is the increasing slope. Another strategy is to implement an exponential increase $r_k=\frac{1}{\beta^{K-k+1}}$, with $\beta$ being the base of the exponential increase, and $K$ the total number of steps. In all cases, the scheduler is a monotone-increasing function. 

In our experiments, we considered $6$ different types of variable compression mechanisms based on the following equation, 

\begin{align}
	c =\min\bigg(c_{max} - a \frac{ c_{max} - c_{min}}{K}k, c_{min} \bigg)
\end{align}
We considered the slope $a=\{2,3,4,5,6,7\}$ and in all cases $c_{max}=128$, $c_{min}=1$. 
%In Figure \ref{fig:comm_rate_per_epoch} we show the communication rate per epoch for each mechanism. In Figure \ref{fig:comm_floats_per_epoch}, compute the number of communicated floating points per edge in a GNN with feature size $128$ and $256$ activations in the middle layers. In Figure \ref{fig:aggregated_comm_floats} we compute the total number of floating points communicated throughout a whole training session of $300$ epochs.
% \begin{figure*}
% 	\begin{subfigure}{0.33\textwidth}
% 		\centering
% 		\includegraphics[width = \textwidth]{figures/schedulers/comm_rate_per_epoch.pdf} 
% 		\caption{Compression rate per epoch}
% 		\label{fig:comm_rate_per_epoch}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.33\textwidth}
% 		\includegraphics[width = \textwidth]{figures/schedulers/comm_floats_per_epoch.pdf} 
% 		\caption{Floating points comm. per epoch}
% 		\label{fig:comm_floats_per_epoch}
% 	\end{subfigure}
% 	\begin{subfigure}{0.33\textwidth}
% 		\includegraphics[width = \textwidth]{figures/schedulers/comm_floats_aggregated.pdf} 
% 		\caption{Aggregated floating points}
% 		\label{fig:aggregated_comm_floats}
% 	\end{subfigure}
% 	\caption{Compression rate and floating point communicated per epoch.}
% \end{figure*}
% \begin{figure*}
% 	\begin{subfigure}{0.5\textwidth}
% 		\centering
% 		\includegraphics[width = \textwidth]{figures/AccVsEdges/Acc_vs_PercentCrossEdges_arxiv.pdf} 
% 		\caption{Arxiv}
% 		\label{fig:acc_vs_self_arxiv}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.5\textwidth}
% 		\includegraphics[width = \textwidth]{figures/AccVsEdges/Acc_vs_PercentCrossEdges_prods.pdf} 
% 		\caption{Products}
% 		\label{fig:acc_vs_self_prods}
% 	\end{subfigure}
% 	\caption{Accuracy as a function of the percentage of self-edges.}
%         \label{fig:acc_vs_self}
% \end{figure*}

%\begin{figure*}[h]
%	\centering
%	\begin{subfigure}{0.33\textwidth}
%		\includegraphics[width = \textwidth]{figures/schedulers/comm_rate_per_epoch.pdf} 
%		\subcaption{Compression rate per epoch}
%		\label{fig:comm_rate_per_epoch}
%	\end{subfigure}
%	\begin{subfigure}{0.33\textwidth}
%		\includegraphics[width = \textwidth]{figures/schedulers/comm_floats_per_epoch.pdf} 
%		\subcaption{Floating points per epoch}
%		\label{fig:comm_floats_per_epoch}
%	\end{subfigure}
%	\caption{\ref{fig:comm_rate_per_epoch} Compression rate per epoch for the difference training mechanisms. \ref{fig:comm_floats_per_epoch} Floating point numbers communicated per epoch per edge between machines.}
%\end{figure*}



%\begin{figure}
%	\centering
%	\includegraphics[width =0.6 \textwidth]{figures/schedulers/comm_floats_aggregated.pdf} 
%	\caption{Accumulated number of communicated floating point numbers for the different training mechanisms.}
%	\label{fig:accuracy_epoch}
%\end{figure}



%\subsection{Robustness to Scheduler Selection}
%Our Algorithm $\algo$ shows a solid robustness to the choice of slope $a$, and $c_{min}$. We tested several choices of schedulers, and in all of them, the accuracy of the learned GNN matches the one of the no communication, at a fraction of the time. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packag
\section{Partitioning Details}
In all cases, the partitions had the same number of nodes in each partition. In Table \ref{table:edges} we show the number of edges in each server, and across servers. As can be seen, the number of cross edges in METIS partitioning is always smaller than random, which makes sense given the objective of the METIS algorithm. Another important aspect is that as the number of partitions increases, the cross-partition number of edges increases and correspondingly the self-partition decreases. This is why the degradation happens, local graphs are smaller, and more communication is needed.
\input{table_edges.tex}

% \section{Cross-Edge analysis}

% In this section, we study the impact of the number of cross-edges on the graph. We define a cross-edge, as an edge that connects two nodes that are in different partitions. 
% Note that the number of cross-edges is related to the number of partitions, the density of the graph, and the partition method. In Table \ref{table:edges} we show the number and percentage of cross-edges for each graph, partition method, and number of partitions. It can be seen that METIS has a significantly smaller number of cross-edges than random. It is also true, that the number of cross-edges increases with the number of partitions. Also note that OGBN-Products graph has an average degree of $25.26$, whereas OBGN-Papers $6.89$. This means that OBGN-Products graph is sparser than Arxiv, which explains why using METIS partitioning, the number of cross-edges is smaller in OGBN-Products. 

% In Figure \ref{fig:acc_vs_self}, we plot the accuracy as a function of the percentage of cross-edges. In this plot, we only considered the percentage of cross-edges, and therefore, the partition mechanism and number of partitions are not stated. But, as Table \ref{table:edges} indicates, the $4$ points with more cross-edges correspond to random partitions with $2,4,8,16$ partitions respectively. Likewise, the $4$ points with less cross-edges correspond to METIS partitioning with $2,4,8,16$ partitions respectively.



% A salient conclusion is that in all cases as the percentage of self-edges increases the accuracy deteriorates. This is related to the fact that as the number of cross-edges increases the local data is less representative of the whole graph. Another conclusion drawn from \ref{fig:acc_vs_self} is that there is an ordering in the performance from less compression (Fixed compression $2$) to no communication. This is related to the fact that less compression transmits more information over the cross-edges than no communication. The greater the number of cross-edges the role of the compression becomes more important. 

% Regarding the sparsity of the datasets, the two plots show different behaviors. In the sparser graph \ref{fig:acc_vs_self_arxiv}, as edges are added, the accuracy increases, almost linearly. This is related to the fact that the cross-edges are not redundant in the information they bring from far-away nodes. On the other hand, the denser graph \ref{fig:acc_vs_self_prods} shows a saturation around $50\%$. That is to say, as the number of cross-edges decreases below $50\%$, the improvement in accuracy is not significant. This is related to the density of the graph, as given its large degree, nodes tend to be redundant. 



\subsection{Accuracy}
The variable compression mechanism recovers the no communication accuracy in all cases considered. There is no difference in the accuracy obtained with variable compression, and full communication. This is true, for all numbers of servers considered, and all partitions as can be seen in Tables \ref{table:results_random}, and \ref{table:results_metis} for METIS and random partition respectively. 

\input{table_random.tex}

\input{table_metis.tex}


\subsection{Proof of Proposition \ref{prop:fixed_compression}}
\label{appendix:proposition_fixed_compression}
To begin with, we need to show these three lemmas. 

\begin{lemma}[GNN Function Difference]\label{lemma:func_diff}
	Under the assumptions of Proposition \ref{prop:fixed_compression}, the output of an $L$-layer GNN with $F$ and coefficients and $K$ filter taps per layer can be bounded by,
	\begin{align}
		||\Phi(\bbX_1,\bbS;\ccalH) - \Phi(\bbX_2,\bbS;\ccalH)  ||\leq \lambda_{\max}^L ||\bbX_1-\bbX_2||
	\end{align}
\end{lemma}
\begin{proof}[of Lemma \ref{lemma:func_diff}]
	Starting with the first layer of the GNN, and considering a single feature $||\bbx_{l1}-\bbx_{l2}||$, we can look into the difference between the successive layers as follows, 
	\begin{align}
		||\bbX_{l1}-\bbX_{l2}|| =& ||\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbx_{1}\bigg)-\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{2}\bigg)||\\
		\leq& ||\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}-\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{2}||\label{eqn:normalized_lips}\\
  % \text{ normalized Lipschitz assumption \ref{as:normalized_lipschitz}}\\
		\leq& \lambda_{\max}||\bbX_{1}-\bbX_{2}|| \label{eqn:normalized_lips_filters}%\text{ normalized filters assumption \ref{as:filter_bounded}}
	\end{align}
 Where \eqref{eqn:normalized_lips} holds by normalized Lipschitz assumption \ref{as:normalized_lipschitz}, and \eqref{eqn:normalized_lips_filters} holds by the normalized filters assumption \ref{as:filter_bounded}
	By repeating the recursion over $L$ layers we attain the desired result.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}[GNN Gradient Difference]\label{lemma:grad_diff} Under the assumptions of Proposition \ref{prop:fixed_compression}, the output of an $L$-layer GNN with $F$ and coefficients and $K$ filter taps per layer can be bounded by,
	\begin{align}
		&||\nabla_\ccalH\Phi(\bbX_1,\bbS;\ccalH) - \nabla_\ccalH\Phi(\bbX_2,\bbS;\ccalH)  ||\\
  &\leq 2\lambda_{\max} \sqrt{KFL} ||\bbX_1-\bbX_2|| \nonumber
	\end{align}
\end{lemma}
\begin{proof}[of Lemma \ref{lemma:grad_diff}]
	Starting with the first layer of the GNN, note that the derivative of the GNN with respect to any parameter in the first layer is the value of the polynomial. By denoting $h_v$ an element on the first layer of the GNN, the derivative with respect to the first layer is, 
	\begin{align}
		\nabla_{h_v}\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}\bigg)=\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}\bigg)  \bbS^k\bbX_{1}.
	\end{align}
	By taking the difference we get, 
	\begin{align}
		&||\nabla_{k_v}\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbx_{1}\bigg)-\nabla_{k_v}\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{2}\bigg)||\\
		&=||\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}\bigg)  \bbS^k\bbX_{1}-\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{2}\bigg)  \bbS^k\bbX_{2}||\\
		&\leq ||\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}\bigg) \bigg(  \bbS^k\bbX_{1}- \bbS^k\bbX_{2}\bigg)||\\
  % \text{ triangle inequality }\\
		&+||\bigg(\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}\bigg) -\non\bigg( \sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{2}\bigg)\bigg)  \bigg( \bbS^k\bbX_{2}\bigg)||
	\end{align}
	Now, given that the activation is normalized Lipschitz by assumption \ref{as:normalized_lipschitz}, the signals are normalized, and that the filter is normalized by assumption \ref{as:filter_bounded}, we can bound this term by, 
	\begin{align}
		&||\nabla_{k_v}\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{1}\bigg)-\nabla_{k_v}\non\bigg(\sum_{k=0}^{K-1}\bbH_k \bbS^k\bbX_{2}\bigg)||\nonumber\\
  &\leq 2 \lambda_{\max}||\bbX_1 - \bbX_2 || 
	\end{align}
	By repeating the previous result for all layers, and all features and considering that the GNN has $KFL$ coefficients, we complete the proof. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Lipschitz Gradients with respect to the parameters]\label{lemma:lipschitz_loss_wrt_params}Under the assumptions of Proposition \ref{prop:fixed_compression}, the output of an $L$-layer GNN with $F$ and coefficients and $K$ filter taps per layer can be bounded by,
	\begin{align}
		&||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1)) - \nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))  ||\nonumber\\
  &\leq 2ML ||\ccalH_1-\ccalH_2||
	\end{align}
\end{lemma}
\begin{proof}[of Lemma \ref{lemma:lipschitz_loss_wrt_params}] We begin by using the chain rule as follows, 
	\begin{align}
		&||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1)) - \nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))  ||\\
		&=||\nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1))\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_1)\nonumber\\
  &- \nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_2)  ||\\
		&\leq||\bigg(\nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1)) \nonumber \\&-\nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))\bigg)\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_2)  ||\\% \text{ triangle inequality }\\
		&+||\bigg(\nabla_\ccalH\Phi(\bbx_1,\bbS;\ccalH)-\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_2)\bigg)\nonumber\\
  &\nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1))|| 
	\end{align}
	Note that we consider the filters $\ccalH$ as a vector, where the coefficients have been concatenated. We can now use Cauchy-Schwartz to obtain, 
	\begin{align}
		&||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1)) - \nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))  ||\\
		&\leq||\nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1))- \nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))|| \nonumber\\
  &||\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_2)  || \\
		&+||\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_1)-\nabla_\ccalH\Phi(\bbx,\bbS;\ccalH_2)|| \nonumber\\
  &||\nabla\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1))|| .
	\end{align}
	We can now use Assumptions \ref{as:Loss_Grad_Lipschitz}, and \ref{as:GNN_lipschitz}, to obtain
	\begin{align}
		&||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_1)) - \nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_2))  ||\nonumber\\
  &\leq 2ML ||\ccalH_1-\ccalH_2||
	\end{align}
	By denoting $L_\nabla = 2ML$ we complete the proof. 
	% Now we can use Assummption \ref{as:Loss_Grad_Lipschitz} and Assumption \ref{lemma:lipschitz_loss_wrt_params} for the first term, and Lemma \ref{lemma:grad_diff} for the second term to obtain, 
	% \begin{align}
		%     &||\nabla_\ccalH\ell(\bby,\Phi(\bbx_1,\bbS;\ccalH)) - \nabla_\ccalH\ell(\bby,\Phi(\bbx_2,\bbS;\ccalH))  ||\\
		% &\leq||\nabla\ell(\bby,\Phi(\bbx_1,\bbS;\ccalH))- \nabla\ell(\bby,\Phi(\bbx_2,\bbS;\ccalH))|| ||\nabla_\ccalH\Phi(\bbx_2,\bbS;\ccalH)  || \\
		% &+||\nabla_\ccalH\Phi(\bbx_1,\bbS;\ccalH)-\nabla_\ccalH\Phi(\bbx_2,\bbS;\ccalH)|| ||\nabla\ell(\bby,\Phi(\bbx_1,\bbS;\ccalH))|| 
		% \end{align}
\end{proof}

\begin{lemma}[Submartingale]\label{lemma:submartingale} 
	Consider the iterates generated by equation \ref{eqn:SGD} where the input vector $\bbx$ is compressed with error $\epsilon$ (cf. Definition \ref{eqn:compress_decompress}). Let the step-size  be $\eta\leq 1/\lipGrad$, if the compression error is such that, 
	\begin{align}\label{eqn:prop_submartingale_condition}
		\mbE_\ccalD[||\nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t)) ||^2] \geq \lipGrad^2\epsilon^2
	\end{align}
	then the iterates satisfy that, 
	\begin{align}
		\mbE[\ell(y,\Phi(x,\bbS;\ccalH_{t+1}))] \leq \mbE[\ell (y,\Phi(x,\bbS;\ccalH_t))]
	\end{align}
\end{lemma}

\begin{proof}[of Lemma \ref{lemma:submartingale}]
	This proof follows the lines of \cite{bertsekas2000gradient}, and we start by defining a continuous function $g(\alpha)$ as follows, 
	\begin{align}
		g(\alpha)=\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t-\alpha\eta_t\nabla\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))))].
	\end{align}
	Note that, $g(0)=\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]$ and 
 
 $g(1)=\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))]$, and also that the integral of $\frac{\partial}{\partial \alpha} g(\alpha)$ satisfies
	\begin{align}
		&g(1)-g(0)\nonumber\\
  &= \int_{0}^1 \frac{\partial}{\partial \alpha} g(\alpha) d\alpha \\
		&=  - \mbE[\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))^\intercal \int_{0}^1\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t\nonumber\\
  &-\alpha\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))))d\alpha]\label{eqn:prop_submartingale_chain_rule}\\
		&=  - \mbE[\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))^\intercal \int_{0}^1\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t\nonumber\\
  &-\alpha\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))))\\
		&+\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))d\alpha
		]\label{eqn:prop_submartingale_chain_rule_add_subtract}\\
		&=- \mbE[\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) ^\intercal\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))\nonumber\\
		&+\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) ^\intercal\int_{0}^1\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t\nonumber\\
  &-\alpha\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))))\nonumber\\
		&\quad\quad\quad\quad\quad-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))d\alpha
		]\label{eqn:prop_submartingale_chain_rule_add_subtract_organize},
	\end{align}
	where \eqref{eqn:prop_submartingale_chain_rule} holds by the chain rule, \eqref{eqn:prop_submartingale_chain_rule_add_subtract} holds as we are adding and subtracting the same term, and \eqref{eqn:prop_submartingale_chain_rule_add_subtract_organize} is a rearrangement of terms. We can now utilize Cauchy-Schwartz to bound the difference as follows, 
	
	\begin{align}
		&\mbE_\ccalD[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\nonumber\\
		&\leq - \mbE[\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) ^\intercal\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))\\
		&+\frac{\eta }{2}||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))|| || \nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t\nonumber\\
  &-\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t))))\nonumber\\
  &-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||.\nonumber 
	\end{align}
	where the previous inequality holds given that $\int_0^1 \alpha^2 d\alpha=\frac{1}{2}$. We can utilize Lemma \ref{lemma:lipschitz_loss_wrt_params} to bound the difference between the gradients as follows, 
	\begin{align}
		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		&\leq \mbE[-\eta\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) ^\intercal\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))\nonumber\\
  &+\frac{\lipGrad\eta ^2}{2}||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))||^2]\nonumber .
	\end{align}
	% \red{hereRESUME}
	% Now, we can rearrange as follows,
	% \begin{align}
		%    &\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		%    &\leq - \mbE[2\langle\sqrt{\frac{\lipGrad\eta ^2}{2}}\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) , \sqrt{\frac{2}{\lipGrad}}\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t)) \rangle\\
		%    &+\langle\sqrt{\frac{\lipGrad\eta ^2}{2}}\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t)),\sqrt{\frac{\lipGrad\eta ^2}{2}}\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))\rangle]\nonumber .
		% \end{align}
	% Knowing that for any two vectors, $\bba,\bbb$, $||\bba-\bbb||^2-||\bbb||^2=||\bba||^2-2\bba^\intercal\bbb$ given that the norm is induced by the inner product we obtain, 
	% \begin{align}
		%    &\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		%    &\leq - \frac{2}{\lipGrad}\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2-||\frac{\lipGrad\eta }{2}\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) - \nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t)) ||^2]\nonumber .
		% \end{align}
	
	
	% \red{here}
	
	Now, we can factor $-\eta/2$, and we obtain, 
	\begin{align}
		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		&\leq  \frac{-\eta}{2}\mbE[2\nabla_\ccalH\ell(\bby,\Phi(\tilde \bbx,\bbS;\ccalH_t)) ^\intercal\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))\nonumber\\
  &-||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))||^2] \\
		&+\mbE[\frac{\lipGrad\eta ^2-\eta}{2}||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))||^2]\nonumber .
	\end{align}
	Now by imposing the condition that $\eta<\frac{1}{\lipGrad}$, the second term can be ignored. Knowing that for any two vectors, $\bba,\bbb$, $||\bba-\bbb||^2-||\bbb||^2=||\bba||^2-2\bba^\intercal\bbb$ given that the norm is induced by the inner product we obtain, 
	\begin{align}
		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\nonumber\\
		&\leq  \frac{-\eta}{2}\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2\nonumber\\
  &-||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2] \nonumber .
	\end{align}
%	Now, we can partition the last element by adding and subtracting $\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))$ as follows, 
%\begin{align}
%&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\label{eqn:add_subtract}\\
%		&\leq  \frac{-\eta}{2}\bigg(\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2] \nonumber\\
%  &-\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))+\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2]\bigg)\nonumber .
%	\end{align}
% Now note that we consider the vectorized tensor $\ccalH$, and the norm in \ref{eqn:add_subtract} is induced by the innner product. Therefore, we can use the property $||a+b||^2\leq 3||a||^2 + 3||b||^2$, as follows,  
%	\begin{align}
%		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
%		&\leq  \frac{-\eta}{2}\bigg(\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2]-3\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\tilde\bbx,\bbS;\ccalH_t))-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2] \\
%		&-3\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))-\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2]\bigg)\nonumber ,
%	\end{align}
%	note that the cross terms are equal to zero given that $\mbE[\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]=\mbE[\nabla_\ccalH \ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]$. 
 Finally, by Lemma \ref{lemma:grad_diff}, and compression mechanism \ref{def:CompressionDecompression}, 
	\begin{align}
		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k+1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		&\leq  \frac{-\eta}{2}\bigg(\mbE[||\nabla_\ccalH\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))||^2]-\lipGrad^2\epsilon^2\bigg)\nonumber .
	\end{align}
	
	
	By imposing the condition in \ref{eqn:prop_submartingale_condition} we complete the proof. 
\end{proof}


\begin{proof}[of Proposition \ref{prop:fixed_compression}]
	To begin with, for every $\beta$ we define the stopping time $K$ as
	\begin{align}
		K=\min_{k\geq 0} \{\mbE[||\nabla_\ccalH \ell(y,\Phi(\bbx,\bbS;\ccalH_t)) ||^2\leq \lipGrad^2\epsilon_k^2 +\beta^2]\}
	\end{align}
	We need to show that $\mbE[k^*]$ is of order $\ccalO(1/\beta)$. To do so, we start by taking the difference between the last iterate $K$ and the first one as follows, 
	\begin{align}
		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_0))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		&=\mbE_{K}[\mbE[\sum_{k=1}^K\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k-1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t)) ]]\\
		&=\sum_{t=0}^\infty\mbE[\sum_{k=1}^t\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k-1}))\nonumber\\
  &-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t)) ]P(K=t)\label{eqn:propostion_convergence_termwise_summation}
	\end{align}
	Now, we know that for all $t\leq K$, we have that, 
	\begin{align}
		\mbE[\sum_{k=1}^K\ell(\bby,\Phi(\bbx,\bbS;\ccalH_{k-1}))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t)) ]\geq \eta \beta .\label{eqn:propostion_convergence_difference_beta}
	\end{align}
	We can now substitute condition \ref{eqn:propostion_convergence_difference_beta} into equation \ref{eqn:propostion_convergence_termwise_summation} to obtain, 
	\begin{align}
		&\mbE[\ell(\bby,\Phi(\bbx,\bbS;\ccalH_0))-\ell(\bby,\Phi(\bbx,\bbS;\ccalH_t))]\\
		&\geq\sum_{t=0}^\infty \eta \beta K P(K=t)\geq\beta \eta \mbE[K].
	\end{align}
	Given that the loss function is non-negative, and dividing in both sides of the previous inequality by $\beta \eta$, we complete the proof.
	
\end{proof}



\section{Proof of Proposition \ref{prop:scheduler}}\label{appendix:proof_scheduler}
The sketch of this proof is as follows, first, we construct a martingale by multiplying the norm of the gradient by the condition that we want to satisfy. Second, we show that this construction is effectively a martingale. Third, we show that it converges. Finally, we show what the limit of this convergent martingale is.

% To begin with, an alternative way of showing that Proposition \ref{prop:scheduler} is true, is by showing that the inferior limit (i.e. $\lim\inf$) of the expected value of the norm of the gradient is $0$. Therefore, if 
% \begin{align}
	% 	\lim \inf_{k\to\infty} \mbE[|| \nabla_\ccalH \ell (y,\phi(x,\bbS;\ccalH_t))] = 0,
	% \end{align}
% we can show by contradiction, that for every $\delta>0$, and $k_0$, there exists a value of $K$, such that $\mbE[|| \nabla_\ccalH \ell (y,\phi(x,\bbS;\ccalH_t))] \leq \delta$. 

To begin with, we define the filtration $\ccalF_t$ by iterates generated according to \eqref{eqn:SGD}, and the sequence $X_t$ as follows, 
\begin{align}
	&X_t = ||\nabla_\ccalH \ell (y,\phi(x,\bbS;\ccalH_t))||^2\bbone[||\nabla_\ccalH \ell (y,\phi(x,\bbS;\ccalH_t))||^2\nonumber\\
 &\geq  L_\nabla^2 \epsilon^2_{t^{'}}+\sigma, t^{'}\leq t],
\end{align}
where $\bbone[\cdot]$ is the indicator function. The expected value of $|X_t|$ is bounded by Assumption \ref{as:Loss_Grad_Lipschitz}, and $X_t$ is adapted to the filtration generated by the iterates of \ref{eqn:SGD}. 
By \cite{durrett2019probability}, to show that $X_n$ is a super-martingale, we require, 
\begin{align}
	\mbE[X_{t+1}|\ccalF_t]\leq X_t.
\end{align}
By contradiction, we can argue that $\mbE[X_{t+1}|\ccalF_t]> X_t$. Now, if $\mbE[X_{t+1}|\ccalF_t]> X_t$ for every $t>t_0$, then it must be the case that $X_{t_0}>L_\nabla^2 \epsilon^2_{t_0}+\sigma$. If this is not the case, the indicator function will make the sequence equal to $0$ for all $t>t_0$, disproving the contradiction. Given that $X_{t_0}>L_\nabla^2 \epsilon^2_{t^{'}}+\sigma$, we can fix $\epsilon_0$, and by Proposition \ref{prop:fixed_compression}, we arrive at a contradiction, and therefore $X_n$ is a super-martingale.

Given that the $X_t$ is bounded below by $0$, by the  Martingale Convergence Theorem \cite[Theorem 4.2.1]{durrett2019probability}, $X_t$ converges. 

Now it remains to show that the limit of $\lim_{t\to\infty}X_t=0$. We can assume that this is not true. We can therefore assume that $\lim_{t\to\infty}X_t=A<\infty$ with $A>\sigma$. Now, we know that the scheduler decreases, and therefore $\exists t^*:L^2_\nabla\epsilon^2_{t^*}+\sigma<A$. In this case again, $X_t$ cannot be larger that  $L^2_\nabla\epsilon^2_{t^*}+\sigma$ forever by Proposition \ref{prop:fixed_compression}. Therefore, there is no $A>\sigma$ such that $X_t$ converges to. Which implies that $X_n\to 0$.

To finalize, given that we made no assumptions over the initial time $t$, $||\nabla_\ccalH \ell (y,\phi(x,\bbS;\ccalH_t))||^2\leq \sigma$ happens infinitely often, completing the proof. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% \section{Varying Compression Rates}


% In this work, we propose to utilize varying compression rates while learning the GNN. At the beginning of the training, we utilize a large compression ratio and we reduce it as we train. Intuitively, our method proposes to increase the fidelity of the estimator of the gradient as the GNN approaches convergence. 

% Given that edges of the graph $\bbS$ exist between nodes own between different workers, when a gradient at worker $W_i$ is computed, data from workers $W_j$'s that posses nodes adjacent to the ones owned by $W_i$ are needed. In order to reduce the communication costs between workers, we propose to compress the information sent between them. To this end, in this work, we propose to communicate the activation between agents.
% %
% \begin{definition}
	% 	The compression and decompression mechanism $g_{\epsilon,r},g_{\epsilon^{-1},r}$ with compression error $\epsilon$, and rate $r$,  satisfies that given a set of parameters $x$, when compressed and decompressed, the following relation holds i.e.,
	% 	\begin{align}
		% 		&a= g_{\epsilon,r} (x),  \text{ and }\tilde x = g_\epsilon^{-1}(g_\epsilon( x)) \text{ and }\mbE[\tilde x - x]=0 \text{ with }\mbE[||\tilde x - x||^2]\leq\epsilon^2,
		% 	\end{align}
	% 	where $a\in \reals^m$ is the compressed signal with rate $r$, $\frac{m}{n}=r$. If $\epsilon=0$ we say that we compute a loss-less compression. 
	% \end{definition}
% %

% Returning to \ref{eqn:SGD}, in this paper we propose to make the updates on $\ccalH$ based on the decompressed signals $\tilde x$. To this end, each $W_i$ compresses its activation to obtain $a_j$. Next, it transmits the compressed activation to its neighbours. Upon receiving all the compressed activations, each worker decompresses  the information and computes the backward pass to obtain the stochastic gradient with which it computes the gradient step. In this paper, we propose to vary the compression rate $r_k$ across iterations. A more succinct description of the procedure can be found in Algorithm \ref{alg:varying_compr_rates}.




% The advantage of our procedure relies on the fact that the compressed data is transmitted between agents reducing the costs of communication. Given that the bottleneck is given by the communication times, compressing and decompressing information locally adds less overhead than transmitting the vector $x$.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %\section{experiments}


% \section{Algorithm Convergence}
% In order to show convergence of Algorithm \ref{alg:varying_compr_rates}, we need to introduce three assumptions. 

% \begin{assumption}
	% 	The positive loss $\ell$ function has $L$ Lipschitz continuous gradients i.e., $||\nabla\ell(\bby_1,\bbz)- \nabla\ell(\Phi(\bby_2,\bbz)||\leq L||\bby_1-\bby_2||$.
	% \end{assumption}
% \begin{assumption}
	% 	The empirical estimator of the gradient $ \nabla_\ccalH \ell (y_i,\Phi(x_i,\bbS;\ccalH_t))$ is an unbiased estimator of the gradient $\nabla_\ccalH \ell (y_i,\Phi(x_i,\bbS;\ccalH_t))$, and the variance can be controlled by the number of samples in the batch $B$ as follows, 
	% 	\begin{align}
		% 		\mbE\bigg[|| \frac{1}{B}\sum_{i=1}^B\nabla_\ccalH \ell (y_i,\Phi(x_i,\bbS;\ccalH_t))- \nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t))||^2\bigg]\leq \frac{\sigma^2}{B}
		% 	\end{align}
	% 	with $\infty>\sigma>0$ being the variance of the estimator. 
	% \end{assumption}
% \begin{assumption}
	% 	The graph convolutional filters in every layer of the graph neural network are bounded, i.e.
	% 	\begin{align}
		% 		||h_{*\bbS}x|| \leq ||x|| \lambda_{max} \bigg(\sum_{t=0}^T h_t \bbS^t\bigg),\text{ with } \lambda_{max} \bigg(\sum_{t=0}^T h_t \bbS^t\bigg)<\infty.
		% 	\end{align}
	% \end{assumption}


% Proposition \ref{prop:compression_rate} shows that the iterates generated by Algorithm \ref{alg:varying_compr_rates} form a supermartingale. If we run Algorithm \ref{alg:varying_compr_rates} sufficiently it will converge to a first order stationary point. Intuitively, as we reduce the value of the loss $\ell$, so does the magnitude of the gradient $||\nabla_\ccalH \ell (f_{\ccalH_{k+1}})||$, therefore, as we increase the number of epochs, we need to reduce the compression error. 


% \begin{proposition}[Convergence of \algo]
	% 	Consider the iterates generates by equation \ref{eqn:compressed_SGD} where the gradient is compressed with compression rate $r_k$ (cf. Definition \ref{eqn:compress_decompress}). Let the step-size  be $\eta\leq 1/\lipGrad$, if the compression error is such that at every step $k$, 
	% 	\begin{align}
		% 		\mbE[||\nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t)) ||^2] \geq \frac{\lipGrad^2\epsilon_k^2}{B}+\frac{\sigma^2}{B} + \beta,
		% 	\end{align}
	% 	Then \algo\ converges to a first order stationary point in $K\leq \ccalO(\frac{1}{\beta})$ iterations. ,i.e., 
	% 	\begin{align}
		% 		\mbE[|| \nabla_\ccalH \ell (y,\Phi(x,\bbS;\ccalH_t))||^2]\leq \beta^2
		% 	\end{align}
	% \end{proposition}



% \subsection{Scheduler} \label{subsec:scheduler}


